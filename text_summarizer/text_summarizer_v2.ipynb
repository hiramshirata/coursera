{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49",
   "display_name": "Python 3.8.8 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 importing libs\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import textwrap\n",
    "wrapper = textwrap.TextWrapper(width=70)\n",
    "\n",
    "import trax\n",
    "from trax import layers as tl\n",
    "from trax.fastmath import numpy as jnp\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# to print the entire np array\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stream_fn = trax.data.TFDS('cnn_dailymail',\n",
    "                                 data_dir='data/',\n",
    "                                 keys=('article', 'highlights'),\n",
    "                                 train=True)\n",
    "\n",
    "# This should be much faster as the data is downloaded already.\n",
    "eval_stream_fn = trax.data.TFDS('cnn_dailymail',\n",
    "                                data_dir='data/',\n",
    "                                keys=('article', 'highlights'),\n",
    "                                train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocabulary to perform the word to index\n",
    "\n",
    "def tokenize(input_str, EOS=1):\n",
    "    \"\"\"Input str to features dict, ready for inference\"\"\"\n",
    "    # Use the trax.data.tokenize method. It takes streams and returns streams,\n",
    "    # we get around it by making a 1-element stream with `iter`.\n",
    "    inputs =  next(trax.data.tokenize(iter([input_str]),\n",
    "                                      vocab_dir='vocab_dir/',\n",
    "                                      vocab_file='summarize32k.subword.subwords'))\n",
    "    # Mark the end of the sentence with EOS\n",
    "    return list(inputs) + [EOS]\n",
    "\n",
    "def detokenize(integers):\n",
    "    \"\"\"List of ints to str\"\"\"\n",
    "    s = trax.data.detokenize(integers,\n",
    "                             vocab_dir='vocab_dir/',\n",
    "                             vocab_file='summarize32k.subword.subwords')\n",
    "    return wrapper.fill(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special tokens\n",
    "SEP = 0 # Padding or separator token\n",
    "EOS = 1 # End of sentence token\n",
    "\n",
    "# Concatenate tokenized inputs and targets using 0 as separator.\n",
    "def preprocess(stream):\n",
    "    for (article, summary) in stream:\n",
    "        joint = np.array(list(article) + [EOS, SEP] + list(summary) + [EOS])\n",
    "        mask = [0] * (len(list(article)) + 2) + [1] * (len(list(summary)) + 1) # Accounting for EOS and SEP\n",
    "        yield joint, joint, np.array(mask)\n",
    "\n",
    "# You can combine a few data preprocessing steps into a pipeline like this.\n",
    "input_pipeline = trax.data.Serial(\n",
    "    # Tokenizes\n",
    "    trax.data.Tokenize(vocab_dir='vocab_dir/',\n",
    "                       vocab_file='summarize32k.subword.subwords'),\n",
    "    # Uses function defined above\n",
    "    preprocess,\n",
    "    # Filters out examples longer than 4096\n",
    "    trax.data.FilterByLength(4096)\n",
    ")\n",
    "\n",
    "# Apply preprocessing to data streams.\n",
    "train_stream = input_pipeline(train_stream_fn())\n",
    "eval_stream = input_pipeline(eval_stream_fn())\n",
    "\n",
    "train_input, train_target, train_mask = next(train_stream)\n",
    "\n",
    "assert sum((train_input - train_target)**2) == 0  # They are the same in Language Model (LM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 of length < 512. And so on. \n",
    "boundaries =  [128, 256,  512, 1024]\n",
    "batch_sizes = [16,  8,  4,  2,  1]\n",
    "\n",
    "# Create the streams.\n",
    "train_batch_stream = trax.data.BucketByLength(\n",
    "    boundaries, batch_sizes)(train_stream)\n",
    "\n",
    "eval_batch_stream = trax.data.BucketByLength(\n",
    "    boundaries, batch_sizes)(eval_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"A middle-school teacher in China has inked hundreds of sketches that\\nare beyond be-leaf. Politics teacher Wang Lian, 35,  has created 1000\\nstunning ink drawings covering subjects as varied as cartoon\\ncharacters and landscapes to animals, birds according to\\xa0the\\xa0People's\\nDaily Online. The intricate scribbles on leaves feature Wang's\\nfavourite sites across the city of Nanjing, which include the\\nPresidential Palace, Yangtze River Bridge, the ancient Jiming Temple\\nand the Qinhuai River. Natural canvas: Artist and teacher Wang Lian\\nhas done hundreds of drawings, like this temple, on leaves she\\ncollects in the park and on the streets . Delicate: She uses an ink\\npen to gently draw the local scenes and buildings on the dried out\\nleaves . 'Although teaching politics is my job, drawing is my passion\\nand hobby,' said Wang. 'I first tried drawing on leaves about 10 years\\nago and fell in love with it as an art form immediately. 'It's like\\ndrawing on very old parchment paper, you have to be really careful\\nthat you don't damage the leaf because it is very fragile and this\\nhelps focus your attention and abilities.' Wang started giving the\\ndrawings away on\\xa0Christmas Eve in 2012 when her junior high school son\\ncame home saying he wanted to prepare some gifts for his classmates.\\nBeing an avid painter, Wang decided to give her son's friends unique\\npresents of gingko leaf paintings. Wang loves gingko leaves and will\\noften pick them up along Gingko Avenue, near to her school, in Nanjing\\nin east China's Jiangsu province. Every autumn she collects about\\n2,000 leaves from the ground to ensure she has enough to cover spoils\\ntoo. Intricate: Teacher Wang has drawn hundreds of local scenes on\\nleaves she has collected from the park . Hobby: The artist collects\\nleaves every autumn and dries them out so she can sketch these\\nimpressive building scenes . 'The colour and shape of gingko leaves\\nare particularly beautiful,' she said. 'I need to collect around 2000\\nleaves because this will include losses'. She takes them home where\\nshe then presses them between the pages of books. 'Luckily, I have\\nquite a lot of books and I try to use old ones or ones that I've\\nalready read so I don't end up with nothing to read.' Once they are\\ndried, she carefully takes each one and using an ink fountain pen\\ncreates her masterpieces. She said: 'Some people are into capturing\\nbeauty through photography, but for me, a digitalised image just isn't\\nthe same. New leaf:\\xa0Politics teacher Wang Lian has drawn hundreds of\\ndoodles on leaves for the last 10 years . 'By drawing what I see I\\nbecome far more a part of the process and part of the final piece.\\n'One day I hope to be able to put my collection on display, but for\\nnow it's really just for my own pleasure.' Wang's leaf paintings are\\nturned into bookmarks, postcards and sometimes even given as gifts to\\nher her students so she can share the beauty of leaf paintings. But\\nlocals who have had the luck of being able to see Wang's art have been\\ngobsmacked. Local art collector On Hao, 58, said: 'These are truly\\nremarkable and beautiful creations. 'She has so much talent she is\\nwasted in teaching.'<EOS><pad>Worksinclude pictures of Presidential\\nPalace and Yangtze River Bridge . Has inked 1,000 pieces of art on\\nleaves in last two years . Gives work away to students in form of\\nbookmarks and postcards .<EOS><pad><pad><pad><pad><pad><pad><pad><pad>\\n<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\\n<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\\n<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\\n<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\\n<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\\n<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\\n<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\\n<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\\n<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\\n<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\\n<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\\n<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\\n<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\\n<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\\n<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\\n<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\\n<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\\n<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\\n<pad><pad><pad><pad><pad>\""
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "detokenize(next(eval_batch_stream)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TransformerLM = trax.models.TransformerLM(\n",
    "    vocab_size=33300,\n",
    "    d_model=512,\n",
    "    d_ff=2048,\n",
    "    n_layers=6,\n",
    "    n_heads=8,\n",
    "    dropout=0.1,\n",
    "    max_len=4096,\n",
    "    mode='train',\n",
    "    ff_activation=tl.Relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trax.supervised import training\n",
    "\n",
    "model_path = \"/home/yuguro/Desktop/personal/coursera/text_summarizer/model/\"\n",
    "# UNQ_C8\n",
    "# GRADED FUNCTION: train_model\n",
    "def training_loop(TransformerLM, train_gen, eval_gen, output_dir=model_path):\n",
    "    '''\n",
    "    Input:\n",
    "        TransformerLM (trax.layers.combinators.Serial): The model you are building.\n",
    "        train_gen (generator): Training stream of data.\n",
    "        eval_gen (generator): Evaluation stream of data.\n",
    "        output_dir (str): folder to save your file.\n",
    "        \n",
    "    Returns:\n",
    "        trax.supervised.training.Loop: Training loop.\n",
    "    '''\n",
    "    output_dir = os.path.expanduser(output_dir)  # trainer is an object\n",
    "    lr_schedule = trax.lr.warmup_and_rsqrt_decay(n_warmup_steps=1000, max_value=0.01)\n",
    "\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    train_task = training.TrainTask( \n",
    "      labeled_data=train_gen, # The training generator\n",
    "      loss_layer=tl.CrossEntropyLoss(), # Loss function \n",
    "      optimizer=trax.optimizers.Adam(0.01), # Optimizer (Don't forget to set LR to 0.01)\n",
    "      lr_schedule=lr_schedule,\n",
    "      n_steps_per_checkpoint=5\n",
    "    )\n",
    "\n",
    "    eval_task = training.EvalTask( \n",
    "      labeled_data=eval_gen, # The evaluation generator\n",
    "      metrics=[tl.CrossEntropyLoss(), tl.Accuracy()] # CrossEntropyLoss and Accuracy\n",
    "    )\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    loop = training.Loop(TransformerLM,\n",
    "                         train_task,\n",
    "                         eval_tasks=[eval_task],\n",
    "                         output_dir=output_dir)\n",
    "    \n",
    "    return loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "\n",
      "Step    475: Ran 5 train steps in 105.68 secs\n",
      "Step    475: train CrossEntropyLoss | -3691.59619141\n",
      "Step    475: eval  CrossEntropyLoss | -3874.77514648\n",
      "Step    475: eval          Accuracy |  0.04255319\n",
      "\n",
      "Step    480: Ran 5 train steps in 110.42 secs\n",
      "Step    480: train CrossEntropyLoss | -4202.98583984\n",
      "Step    480: eval  CrossEntropyLoss | -4117.95312500\n",
      "Step    480: eval          Accuracy |  0.03773585\n",
      "\n",
      "Step    485: Ran 5 train steps in 134.92 secs\n",
      "Step    485: train CrossEntropyLoss | -4287.93652344\n",
      "Step    485: eval  CrossEntropyLoss | -3691.33544922\n",
      "Step    485: eval          Accuracy |  0.03947368\n",
      "\n",
      "Step    490: Ran 5 train steps in 119.40 secs\n",
      "Step    490: train CrossEntropyLoss | -4577.04589844\n",
      "Step    490: eval  CrossEntropyLoss | -4577.70507812\n",
      "Step    490: eval          Accuracy |  0.02643172\n",
      "\n",
      "Step    495: Ran 5 train steps in 143.92 secs\n",
      "Step    495: train CrossEntropyLoss | -4723.82666016\n",
      "Step    495: eval  CrossEntropyLoss | -4314.51318359\n",
      "Step    495: eval          Accuracy |  0.02985075\n",
      "\n",
      "Step    500: Ran 5 train steps in 85.44 secs\n",
      "Step    500: train CrossEntropyLoss | -4859.03125000\n",
      "Step    500: eval  CrossEntropyLoss | -4823.23974609\n",
      "Step    500: eval          Accuracy |  0.03921569\n",
      "\n",
      "Step    505: Ran 5 train steps in 120.10 secs\n",
      "Step    505: train CrossEntropyLoss | -5118.63134766\n",
      "Step    505: eval  CrossEntropyLoss | -5030.74853516\n",
      "Step    505: eval          Accuracy |  0.03921569\n",
      "\n",
      "Step    510: Ran 5 train steps in 55.97 secs\n",
      "Step    510: train CrossEntropyLoss | -5465.07324219\n",
      "Step    510: eval  CrossEntropyLoss | -5593.26171875\n",
      "Step    510: eval          Accuracy |  0.03105590\n",
      "\n",
      "Step    515: Ran 5 train steps in 113.24 secs\n",
      "Step    515: train CrossEntropyLoss | -5777.76074219\n",
      "Step    515: eval  CrossEntropyLoss | -6493.00390625\n",
      "Step    515: eval          Accuracy |  0.03896104\n",
      "\n",
      "Step    520: Ran 5 train steps in 124.48 secs\n",
      "Step    520: train CrossEntropyLoss | -5950.87744141\n",
      "Step    520: eval  CrossEntropyLoss | -6245.18017578\n",
      "Step    520: eval          Accuracy |  0.03448276\n",
      "\n",
      "Step    525: Ran 5 train steps in 146.82 secs\n",
      "Step    525: train CrossEntropyLoss | -6299.70019531\n",
      "Step    525: eval  CrossEntropyLoss | -6117.16162109\n",
      "Step    525: eval          Accuracy |  0.04123711\n",
      "\n",
      "Step    530: Ran 5 train steps in 87.08 secs\n",
      "Step    530: train CrossEntropyLoss | -5878.60156250\n",
      "Step    530: eval  CrossEntropyLoss | -5380.87060547\n",
      "Step    530: eval          Accuracy |  0.02325581\n",
      "\n",
      "Step    535: Ran 5 train steps in 86.89 secs\n",
      "Step    535: train CrossEntropyLoss | -6821.98437500\n",
      "Step    535: eval  CrossEntropyLoss | -5811.25000000\n",
      "Step    535: eval          Accuracy |  0.02362205\n",
      "\n",
      "Step    540: Ran 5 train steps in 85.09 secs\n",
      "Step    540: train CrossEntropyLoss | -6979.85449219\n",
      "Step    540: eval  CrossEntropyLoss | -7192.64208984\n",
      "Step    540: eval          Accuracy |  0.02739726\n",
      "\n",
      "Step    545: Ran 5 train steps in 88.56 secs\n",
      "Step    545: train CrossEntropyLoss | -7164.57666016\n",
      "Step    545: eval  CrossEntropyLoss | -7527.68994141\n",
      "Step    545: eval          Accuracy |  0.03139013\n",
      "\n",
      "Step    550: Ran 5 train steps in 85.30 secs\n",
      "Step    550: train CrossEntropyLoss | -7720.76464844\n",
      "Step    550: eval  CrossEntropyLoss | -6752.12646484\n",
      "Step    550: eval          Accuracy |  0.02614379\n",
      "\n",
      "Step    555: Ran 5 train steps in 120.43 secs\n",
      "Step    555: train CrossEntropyLoss | -7996.17431641\n",
      "Step    555: eval  CrossEntropyLoss | -7888.24951172\n",
      "Step    555: eval          Accuracy |  0.04081633\n",
      "\n",
      "Step    560: Ran 5 train steps in 149.48 secs\n",
      "Step    560: train CrossEntropyLoss | -7651.34130859\n",
      "Step    560: eval  CrossEntropyLoss | -8485.31738281\n",
      "Step    560: eval          Accuracy |  0.04787234\n"
     ]
    }
   ],
   "source": [
    "# Should take around 1.5 minutes\n",
    "# !rm -f model/model.pkl.gz\n",
    "\n",
    "loop = training_loop(TransformerLM, train_batch_stream, eval_batch_stream)\n",
    "loop.load_checkpoint('teste_20_04')\n",
    "loop.run(150)\n",
    "loop.save_checkpoint('teste_20_04')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "# Get the model architecture\n",
    "model = trax.models.TransformerLM(\n",
    "    vocab_size=33300,\n",
    "    d_model=512,\n",
    "    d_ff=2048,\n",
    "    n_layers=6,\n",
    "    n_heads=8,\n",
    "    dropout=0.1,\n",
    "    max_len=4096,\n",
    "    mode='eval',\n",
    "    ff_activation=tl.Relu)\n",
    "\n",
    "# model = TransformerLM(mode='eval')\n",
    "\n",
    "# Load the pre-trained weights\n",
    "model.init_from_file('model/model.pkl.gz', weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C9\n",
    "def next_symbol(cur_output_tokens, model):\n",
    "    \"\"\"Returns the next symbol for a given sentence.\n",
    "\n",
    "    Args:\n",
    "        cur_output_tokens (list): tokenized sentence with EOS and PAD tokens at the end.\n",
    "        model (trax.layers.combinators.Serial): The transformer model.\n",
    "\n",
    "    Returns:\n",
    "        int: tokenized symbol.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    \n",
    "    # current output tokens length\n",
    "    token_length = len(cur_output_tokens)\n",
    "    # calculate the minimum power of 2 big enough to store token_length\n",
    "    # HINT: use np.ceil() and np.log2()\n",
    "    # add 1 to token_length so np.log2() doesn't receive 0 when token_length is 0\n",
    "    padded_length = 2**int(np.ceil(np.log2(token_length + 1)))\n",
    "\n",
    "    # Fill cur_output_tokens with 0's until it reaches padded_length\n",
    "    padded = cur_output_tokens + [0] * (padded_length - token_length)\n",
    "    padded_with_batch = np.array(padded)[None, :] # Don't replace this 'None'! This is a way of setting the batch dim\n",
    "\n",
    "    # model expects a tuple containing two padded tensors (with batch)\n",
    "    output, _ = model((padded_with_batch, padded_with_batch)) \n",
    "    # HINT: output has shape (1, padded_length, vocab_size)\n",
    "    # To get log_probs you need to index output with 0 in the first dim\n",
    "    # token_length in the second dim and all of the entries for the last dim.\n",
    "    log_probs = output[0, token_length, :]\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return int(np.argmax(log_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C10\n",
    "# Decoding functions.\n",
    "def greedy_decode(input_sentence, model):\n",
    "    \"\"\"Greedy decode function.\n",
    "\n",
    "    Args:\n",
    "        input_sentence (string): a sentence or article.\n",
    "        model (trax.layers.combinators.Serial): Transformer model.\n",
    "\n",
    "    Returns:\n",
    "        string: summary of the input.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    # Use tokenize()\n",
    "    cur_output_tokens = tokenize(input_sentence) + [0]\n",
    "    generated_output = [] \n",
    "    cur_output = 0 \n",
    "    EOS = 1 \n",
    "    \n",
    "    while cur_output != EOS:\n",
    "        # Get next symbol\n",
    "        cur_output = next_symbol(cur_output_tokens, model)\n",
    "        # Append next symbol to original sentence\n",
    "        cur_output_tokens.append(cur_output)\n",
    "        # Append next symbol to generated sentence\n",
    "        generated_output.append(cur_output)\n",
    "        print(detokenize(generated_output))\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return detokenize(generated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test it out on a sentence!\n",
    "test_sentence = \"It was a sunny day when I went to the market to buy some flowers. But I only found roses, not tulips.\"\n",
    "print(wrapper.fill(test_sentence), '\\n')\n",
    "print(greedy_decode(test_sentence, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test it out!\n",
    "sentence_test_nxt_symbl = \"I want to fly in the sky tomorrow and it'll be fun.\"\n",
    "detokenize([next_symbol(tokenize(sentence_test_nxt_symbl)+[0], model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    sentence = \"I want to fly in the sky tomorrow and it'll be fun.\"\n",
    "    cur_output_tokens = tokenize(sentence)+[0]\n",
    "    # cur_output_tokens.append(1628)\n",
    "    token_length = len(cur_output_tokens)\n",
    "    padded_length = 2**int(np.ceil(np.log2(token_length + 1)))\n",
    "\n",
    "    padded = cur_output_tokens + [0] * (padded_length - token_length)\n",
    "    padded_with_batch = np.array(padded)[None, :] # Don't replace this 'None'! This is a way of setting the batch dim\n",
    "\n",
    "    output, _ = model((padded_with_batch, padded_with_batch))\n",
    "    log_probs = output[0, token_length, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'10'"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "detokenize([int(np.argmax(log_probs))])"
   ]
  }
 ]
}