{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49",
   "display_name": "Python 3.8.8 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 importing libs\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import textwrap\n",
    "wrapper = textwrap.TextWrapper(width=70)\n",
    "\n",
    "import trax\n",
    "from trax import layers as tl\n",
    "from trax.fastmath import numpy as jnp\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# to print the entire np array\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download train set\n",
    "# train_stream_fn = trax.data.TFDS('scientific_papers',\n",
    "#                                  data_dir='data/',\n",
    "#                                  keys=('article', 'abstract'),\n",
    "#                                  train=True)\n",
    "# eval_stream_fn = trax.data.TFDS('scientific_papers',\n",
    "#                                  data_dir='data/',\n",
    "#                                  keys=('article', 'abstract'),\n",
    "#                                  train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_stream_fn = trax.data.TFDS('cnn_dailymail',\n",
    "                                 data_dir='data/',\n",
    "                                 keys=('article', 'highlights'),\n",
    "                                 train=True)\n",
    "\n",
    "# This should be much faster as the data is downloaded already.\n",
    "eval_stream_fn = trax.data.TFDS('cnn_dailymail',\n",
    "                                data_dir='data/',\n",
    "                                keys=('article', 'highlights'),\n",
    "                                train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocabulary to perform the word to index\n",
    "\n",
    "def tokenize(input_str, EOS=1):\n",
    "    \"\"\"Input str to features dict, ready for inference\"\"\"\n",
    "    # Use the trax.data.tokenize method. It takes streams and returns streams,\n",
    "    # we get around it by making a 1-element stream with `iter`.\n",
    "    inputs =  next(trax.data.tokenize(iter([input_str]),\n",
    "                                      vocab_dir='vocab_dir/',\n",
    "                                      vocab_file='summarize32k.subword.subwords'))\n",
    "    # Mark the end of the sentence with EOS\n",
    "    return list(inputs) + [EOS]\n",
    "\n",
    "def detokenize(integers):\n",
    "    \"\"\"List of ints to str\"\"\"\n",
    "    s = trax.data.detokenize(integers,\n",
    "                             vocab_dir='vocab_dir/',\n",
    "                             vocab_file='summarize32k.subword.subwords')\n",
    "    return wrapper.fill(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special tokens\n",
    "SEP = 0 # Padding or separator token\n",
    "EOS = 1 # End of sentence token\n",
    "\n",
    "# Concatenate tokenized inputs and targets using 0 as separator.\n",
    "def preprocess(stream):\n",
    "    for (article, summary) in stream:\n",
    "        joint = np.array(list(article) + [EOS, SEP] + list(summary) + [EOS])\n",
    "        mask = [0] * (len(list(article)) + 2) + [1] * (len(list(summary)) + 1) # Accounting for EOS and SEP\n",
    "        yield joint, joint, np.array(mask)\n",
    "\n",
    "# You can combine a few data preprocessing steps into a pipeline like this.\n",
    "input_pipeline = trax.data.Serial(\n",
    "    # Tokenizes\n",
    "    trax.data.Tokenize(vocab_dir='vocab_dir/',\n",
    "                       vocab_file='summarize32k.subword.subwords'),\n",
    "    # Uses function defined above\n",
    "    preprocess,\n",
    "    # Filters out examples longer than 4096\n",
    "    trax.data.FilterByLength(4096)\n",
    ")\n",
    "\n",
    "# Apply preprocessing to data streams.\n",
    "train_stream = input_pipeline(train_stream_fn())\n",
    "eval_stream = input_pipeline(eval_stream_fn())\n",
    "\n",
    "train_input, train_target, train_mask = next(train_stream)\n",
    "\n",
    "assert sum((train_input - train_target)**2) == 0  # They are the same in Language Model (LM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 of length < 512. And so on. \n",
    "boundaries =  [128, 256,  512, 1024]\n",
    "batch_sizes = [16,  8,  4,  2,  1]\n",
    "\n",
    "# Create the streams.\n",
    "train_batch_stream = trax.data.BucketByLength(\n",
    "    boundaries, batch_sizes)(train_stream)\n",
    "\n",
    "eval_batch_stream = trax.data.BucketByLength(\n",
    "    boundaries, batch_sizes)(eval_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C1\n",
    "# GRADED FUNCTION: DotProductAttention\n",
    "def DotProductAttention(query, key, value, mask):\n",
    "    \"\"\"Dot product self-attention.\n",
    "    Args:\n",
    "        query (jax.interpreters.xla.DeviceArray): array of query representations with shape (L_q by d)\n",
    "        key (jax.interpreters.xla.DeviceArray): array of key representations with shape (L_k by d)\n",
    "        value (jax.interpreters.xla.DeviceArray): array of value representations with shape (L_k by d) where L_v = L_k\n",
    "        mask (jax.interpreters.xla.DeviceArray): attention-mask, gates attention with shape (L_q by L_k)\n",
    "\n",
    "    Returns:\n",
    "        jax.interpreters.xla.DeviceArray: Self-attention array for q, k, v arrays. (L_q by L_k)\n",
    "    \"\"\"\n",
    "\n",
    "    assert query.shape[-1] == key.shape[-1] == value.shape[-1], \"Embedding dimensions of q, k, v aren't all the same\"\n",
    "\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    # Save depth/dimension of the query embedding for scaling down the dot product\n",
    "    depth = query.shape[-1]\n",
    "\n",
    "    # Calculate scaled query key dot product according to formula above\n",
    "    dots = jnp.matmul(query, jnp.swapaxes(key, -1, -2)) / jnp.sqrt(depth) #rever o conceito do jnp.swapaxes [dots is sqr]\n",
    "    \n",
    "    # Apply the mask\n",
    "    if mask is not None: # The 'None' in this line does not need to be replaced\n",
    "        dots = jnp.where(mask, dots, jnp.full_like(dots, -1e9))\n",
    "    \n",
    "    # Softmax formula implementation\n",
    "    # Use trax.fastmath.logsumexp of dots to avoid underflow by division by large numbers\n",
    "    # Hint: Last axis should be used and keepdims should be True\n",
    "    # Note: softmax = e^(dots - logsumexp(dots)) = E^dots / sumexp(dots)\n",
    "    logsumexp = trax.fastmath.logsumexp(dots, axis=-1, keepdims=True)\n",
    "\n",
    "    # Take exponential of dots minus logsumexp to get softmax\n",
    "    # Use jnp.exp()\n",
    "    dots = jnp.exp(dots - logsumexp)\n",
    "\n",
    "    # Multiply dots by value to get self-attention\n",
    "    # Use jnp.matmul()\n",
    "    attention = jnp.matmul(dots,value)\n",
    "\n",
    "    ## END CODE HERE ###\n",
    "    \n",
    "    return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C2\n",
    "# GRADED FUNCTION: compute_attention_heads_closure\n",
    "def compute_attention_heads_closure(n_heads, d_head):\n",
    "    \"\"\" Function that simulates environment inside CausalAttention function.\n",
    "    Args:\n",
    "        d_head (int):  dimensionality of heads.\n",
    "        n_heads (int): number of attention heads.\n",
    "    Returns:\n",
    "        function: compute_attention_heads function\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_attention_heads(x):\n",
    "        \"\"\" Compute the attention heads.\n",
    "        Args:\n",
    "            x (jax.interpreters.xla.DeviceArray): tensor with shape (batch_size, seqlen, n_heads X d_head).\n",
    "        Returns:\n",
    "            jax.interpreters.xla.DeviceArray: reshaped tensor with shape (batch_size X n_heads, seqlen, d_head).\n",
    "        \"\"\"\n",
    "        ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "        \n",
    "        # Size of the x's batch dimension\n",
    "        batch_size = x.shape[0]\n",
    "        # Length of the sequence\n",
    "        # Should be size of x's first dimension without counting the batch dim\n",
    "        seqlen = x.shape[1]\n",
    "        # Reshape x using jnp.reshape()\n",
    "        # batch_size, seqlen, n_heads*d_head -> batch_size, seqlen, n_heads, d_head\n",
    "        x = jnp.reshape(x, (batch_size, seqlen, n_heads, d_head))\n",
    "        # Transpose x using jnp.transpose()\n",
    "        # batch_size, seqlen, n_heads, d_head -> batch_size, n_heads, seqlen, d_head\n",
    "        # Note that the values within the tuple are the indexes of the dimensions of x and you must rearrange them\n",
    "        x = jnp.transpose(x, (0, 2, 1, 3))\n",
    "        \n",
    "        # Reshape x using jnp.reshape()\n",
    "        # batch_size, n_heads, seqlen, d_head -> batch_size*n_heads, seqlen, d_head\n",
    "        x = jnp.reshape(x, (batch_size*n_heads, seqlen, d_head))\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    return compute_attention_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C3\n",
    "# GRADED FUNCTION: dot_product_self_attention\n",
    "def dot_product_self_attention(q, k, v):\n",
    "    \"\"\" Masked dot product self attention.\n",
    "    Args:\n",
    "        q (jax.interpreters.xla.DeviceArray): queries.\n",
    "        k (jax.interpreters.xla.DeviceArray): keys.\n",
    "        v (jax.interpreters.xla.DeviceArray): values.\n",
    "    Returns:\n",
    "        jax.interpreters.xla.DeviceArray: masked dot product self attention tensor.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    \n",
    "    # Hint: mask size should be equal to L_q. Remember that q has shape (batch_size, L_q, d)\n",
    "    mask_size = q.shape[-2]\n",
    "\n",
    "    # Creates a matrix with ones below the diagonal and 0s above. It should have shape (1, mask_size, mask_size)\n",
    "    # Notice that 1's and 0's get casted to True/False by setting dtype to jnp.bool_\n",
    "    # Use jnp.tril() - Lower triangle of an array and jnp.ones()\n",
    "    mask = jnp.tril(jnp.ones((1, mask_size, mask_size), dtype=jnp.bool_), k=0)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return DotProductAttention(q, k, v, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C4\n",
    "# GRADED FUNCTION: compute_attention_output_closure\n",
    "def compute_attention_output_closure(n_heads, d_head):\n",
    "    \"\"\" Function that simulates environment inside CausalAttention function.\n",
    "    Args:\n",
    "        d_head (int):  dimensionality of heads.\n",
    "        n_heads (int): number of attention heads.\n",
    "    Returns:\n",
    "        function: compute_attention_output function\n",
    "    \"\"\"\n",
    "    \n",
    "    def compute_attention_output(x):\n",
    "        \"\"\" Compute the attention output.\n",
    "        Args:\n",
    "            x (jax.interpreters.xla.DeviceArray): tensor with shape (batch_size X n_heads, seqlen, d_head).\n",
    "        Returns:\n",
    "            jax.interpreters.xla.DeviceArray: reshaped tensor with shape (batch_size, seqlen, n_heads X d_head).\n",
    "        \"\"\"\n",
    "        ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "        \n",
    "        # Length of the sequence\n",
    "        # Should be size of x's first dimension without counting the batch dim\n",
    "        seqlen = x.shape[-2]\n",
    "        # Reshape x using jnp.reshape() to shape (batch_size, n_heads, seqlen, d_head)\n",
    "        x = jnp.reshape(x,(int(x.shape[0]/n_heads),n_heads,seqlen,d_head))\n",
    "        # Transpose x using jnp.transpose() to shape (batch_size, seqlen, n_heads, d_head)\n",
    "        x = jnp.transpose(x,(0,2,1,3))\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Reshape to allow to concatenate the heads\n",
    "        return jnp.reshape(x, (-1, seqlen, n_heads * d_head))\n",
    "    \n",
    "    return compute_attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C5\n",
    "# GRADED FUNCTION: CausalAttention\n",
    "def CausalAttention(d_feature, \n",
    "                    n_heads, \n",
    "                    compute_attention_heads_closure=compute_attention_heads_closure,\n",
    "                    dot_product_self_attention=dot_product_self_attention,\n",
    "                    compute_attention_output_closure=compute_attention_output_closure,\n",
    "                    mode='train'):\n",
    "    \"\"\"Transformer-style multi-headed causal attention.\n",
    "\n",
    "    Args:\n",
    "        d_feature (int):  dimensionality of feature embedding.\n",
    "        n_heads (int): number of attention heads.\n",
    "        compute_attention_heads_closure (function): Closure around compute_attention heads.\n",
    "        dot_product_self_attention (function): dot_product_self_attention function. \n",
    "        compute_attention_output_closure (function): Closure around compute_attention_output. \n",
    "        mode (str): 'train' or 'eval'.\n",
    "\n",
    "    Returns:\n",
    "        trax.layers.combinators.Serial: Multi-headed self-attention model.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert d_feature % n_heads == 0\n",
    "    d_head = d_feature // n_heads\n",
    "\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    \n",
    "    # HINT: The second argument to tl.Fn() is an uncalled function (without the parentheses)\n",
    "    # Since you are dealing with closures you might need to call the outer \n",
    "    # function with the correct parameters to get the actual uncalled function.\n",
    "    ComputeAttentionHeads = tl.Fn('AttnHeads', compute_attention_heads_closure(n_heads,d_head), n_out=1)\n",
    "        \n",
    "\n",
    "    return tl.Serial(\n",
    "        tl.Branch( # creates three towers for one input, takes activations and creates queries keys and values\n",
    "            [tl.Dense(d_feature), ComputeAttentionHeads], # queries\n",
    "            [tl.Dense(d_feature), ComputeAttentionHeads], # keys\n",
    "            [tl.Dense(d_feature), ComputeAttentionHeads], # values\n",
    "        ),\n",
    "        \n",
    "        tl.Fn('DotProductAttn', dot_product_self_attention, n_out=1), # takes QKV\n",
    "        # HINT: The second argument to tl.Fn() is an uncalled function\n",
    "        # Since you are dealing with closures you might need to call the outer \n",
    "        # function with the correct parameters to get the actual uncalled function.\n",
    "        tl.Fn('AttnOutput', compute_attention_output_closure(n_heads,d_head), n_out=1), # to allow for parallel\n",
    "        tl.Dense(d_feature) # Final dense layer\n",
    "    )\n",
    "\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C6\n",
    "# GRADED FUNCTION: DecoderBlock\n",
    "def DecoderBlock(d_model, d_ff, n_heads,\n",
    "                 dropout, mode, ff_activation):\n",
    "    \"\"\"Returns a list of layers that implements a Transformer decoder block.\n",
    "\n",
    "    The input is an activation tensor.\n",
    "\n",
    "    Args:\n",
    "        d_model (int):  depth of embedding.\n",
    "        d_ff (int): depth of feed-forward layer.\n",
    "        n_heads (int): number of attention heads.\n",
    "        dropout (float): dropout rate (how much to drop out).\n",
    "        mode (str): 'train' or 'eval'.\n",
    "        ff_activation (function): the non-linearity in feed-forward layer.\n",
    "\n",
    "    Returns:\n",
    "        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    \n",
    "    # Create masked multi-head attention block using CausalAttention function\n",
    "    causal_attention = CausalAttention( \n",
    "                        d_model,\n",
    "                        n_heads=n_heads,\n",
    "                        mode=mode\n",
    "                        )\n",
    "\n",
    "    # Create feed-forward block (list) with two dense layers with dropout and input normalized\n",
    "    feed_forward = [\n",
    "        # Normalize layer inputs\n",
    "        tl.LayerNorm(),\n",
    "        # Add first feed forward (dense) layer (don't forget to set the correct value for n_units)\n",
    "        tl.Dense(d_ff),\n",
    "        # Add activation function passed in as a parameter (you need to call it!)\n",
    "        ff_activation(), # Generally ReLU\n",
    "        # Add dropout with rate and mode specified (i.e., don't use dropout during evaluation)\n",
    "        tl.Dropout(rate=dropout, mode=mode),\n",
    "        # Add second feed forward layer (don't forget to set the correct value for n_units)\n",
    "        tl.Dense(d_model),\n",
    "        # Add dropout with rate and mode specified (i.e., don't use dropout during evaluation)\n",
    "        tl.Dropout(rate=dropout, mode=mode)\n",
    "    ]\n",
    "\n",
    "    # Add list of two Residual blocks: the attention with normalization and dropout and feed-forward blocks\n",
    "    return [\n",
    "      tl.Residual(\n",
    "          # Normalize layer input\n",
    "          tl.LayerNorm(),\n",
    "          # Add causal attention block previously defined (without parentheses)\n",
    "          causal_attention,\n",
    "          # Add dropout with rate and mode specified\n",
    "          tl.Dropout(rate=dropout, mode=mode)\n",
    "        ),\n",
    "      tl.Residual(\n",
    "          # Add feed forward block (without parentheses)\n",
    "          feed_forward\n",
    "        ),\n",
    "      ]\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C7\n",
    "# GRADED FUNCTION: TransformerLM\n",
    "def TransformerLM(vocab_size=33300,\n",
    "                  d_model=512,\n",
    "                  d_ff=2048,\n",
    "                  n_layers=6,\n",
    "                  n_heads=8,\n",
    "                  dropout=0.1,\n",
    "                  max_len=4096,\n",
    "                  mode='train',\n",
    "                  ff_activation=tl.Relu):\n",
    "    \"\"\"Returns a Transformer language model.\n",
    "\n",
    "    The input to the model is a tensor of tokens. (This model uses only the\n",
    "    decoder part of the overall Transformer.)\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): vocab size.\n",
    "        d_model (int):  depth of embedding.\n",
    "        d_ff (int): depth of feed-forward layer.\n",
    "        n_layers (int): number of decoder layers.\n",
    "        n_heads (int): number of attention heads.\n",
    "        dropout (float): dropout rate (how much to drop out).\n",
    "        max_len (int): maximum symbol length for positional encoding.\n",
    "        mode (str): 'train', 'eval' or 'predict', predict mode is for fast inference.\n",
    "        ff_activation (function): the non-linearity in feed-forward layer.\n",
    "\n",
    "    Returns:\n",
    "        trax.layers.combinators.Serial: A Transformer language model as a layer that maps from a tensor of tokens\n",
    "        to activations over a vocab set.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    \n",
    "    # Embedding inputs and positional encoder\n",
    "    positional_encoder = [ \n",
    "        # Add embedding layer of dimension (vocab_size, d_model)\n",
    "        tl.Embedding(vocab_size, d_model),\n",
    "        # Use dropout with rate and mode specified\n",
    "        tl.Dropout(rate=dropout, mode=mode),\n",
    "        # Add positional encoding layer with maximum input length and mode specified\n",
    "        tl.PositionalEncoding(max_len=max_len, mode=mode)]\n",
    "\n",
    "    # Create stack (list) of decoder blocks with n_layers with necessary parameters\n",
    "    decoder_blocks = [ \n",
    "        DecoderBlock(d_model, d_ff, n_heads,\n",
    "                 dropout, mode, ff_activation) for _ in range(n_layers)]\n",
    "\n",
    "    # Create the complete model as written in the figure\n",
    "    return tl.Serial(\n",
    "        # Use teacher forcing (feed output of previous step to current step)\n",
    "        tl.ShiftRight(mode=mode), # Specify the mode!\n",
    "        # Add positional encoder\n",
    "        positional_encoder,\n",
    "        # Add decoder blocks\n",
    "        decoder_blocks,\n",
    "        # Normalize layer\n",
    "        tl.LayerNorm(),\n",
    "\n",
    "        # Add dense layer of vocab_size (since need to select a word to translate to)\n",
    "        # (a.k.a., logits layer. Note: activation already set by ff_activation)\n",
    "        tl.Dense(vocab_size),\n",
    "        # Get probabilities with Logsoftmax\n",
    "        tl.LogSoftmax()\n",
    "    )\n",
    "\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trax.supervised import training\n",
    "\n",
    "model_path = \"/home/yuguro/Desktop/personal/coursera/text_summarizer/model/\"\n",
    "# UNQ_C8\n",
    "# GRADED FUNCTION: train_model\n",
    "def training_loop(TransformerLM, train_gen, eval_gen, output_dir=model_path):\n",
    "    '''\n",
    "    Input:\n",
    "        TransformerLM (trax.layers.combinators.Serial): The model you are building.\n",
    "        train_gen (generator): Training stream of data.\n",
    "        eval_gen (generator): Evaluation stream of data.\n",
    "        output_dir (str): folder to save your file.\n",
    "        \n",
    "    Returns:\n",
    "        trax.supervised.training.Loop: Training loop.\n",
    "    '''\n",
    "    output_dir = os.path.expanduser(output_dir)  # trainer is an object\n",
    "    lr_schedule = trax.lr.warmup_and_rsqrt_decay(n_warmup_steps=1000, max_value=0.01)\n",
    "\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    train_task = training.TrainTask( \n",
    "      labeled_data=train_gen, # The training generator\n",
    "      loss_layer=tl.CrossEntropyLoss(), # Loss function \n",
    "      optimizer=trax.optimizers.Adam(0.01), # Optimizer (Don't forget to set LR to 0.01)\n",
    "      lr_schedule=lr_schedule,\n",
    "      n_steps_per_checkpoint=5\n",
    "    )\n",
    "\n",
    "    eval_task = training.EvalTask( \n",
    "      labeled_data=eval_gen, # The evaluation generator\n",
    "      metrics=[tl.CrossEntropyLoss(), tl.Accuracy()] # CrossEntropyLoss and Accuracy\n",
    "    )\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    loop = training.Loop(TransformerLM,\n",
    "                         train_task,\n",
    "                         eval_tasks=[eval_task],\n",
    "                         output_dir=output_dir)\n",
    "    \n",
    "    return loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'loop' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-6c7a1fda3a7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'teste_20_04'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'loop' is not defined"
     ]
    }
   ],
   "source": [
    "loop.load_checkpoint('teste_20_04')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Step     15: Ran 5 train steps in 138.27 secs\n",
      "Step     15: train CrossEntropyLoss | -0.30807418\n",
      "Step     15: eval  CrossEntropyLoss | -0.37803090\n",
      "Step     15: eval          Accuracy |  0.02643172\n",
      "\n",
      "Step     20: Ran 5 train steps in 184.45 secs\n",
      "Step     20: train CrossEntropyLoss | -0.43304604\n",
      "Step     20: eval  CrossEntropyLoss | -0.46770573\n",
      "Step     20: eval          Accuracy |  0.02985075\n",
      "\n",
      "Step     25: Ran 5 train steps in 87.95 secs\n",
      "Step     25: train CrossEntropyLoss | -0.58637315\n",
      "Step     25: eval  CrossEntropyLoss | -0.72346008\n",
      "Step     25: eval          Accuracy |  0.03921569\n",
      "\n",
      "Step     30: Ran 5 train steps in 109.69 secs\n",
      "Step     30: train CrossEntropyLoss | -0.80278426\n",
      "Step     30: eval  CrossEntropyLoss | -0.94810319\n",
      "Step     30: eval          Accuracy |  0.01960784\n",
      "\n",
      "Step     35: Ran 5 train steps in 84.50 secs\n",
      "Step     35: train CrossEntropyLoss | -1.11899507\n",
      "Step     35: eval  CrossEntropyLoss | -1.38884628\n",
      "Step     35: eval          Accuracy |  0.03726708\n",
      "\n",
      "Step     40: Ran 5 train steps in 108.55 secs\n",
      "Step     40: train CrossEntropyLoss | -1.55960488\n",
      "Step     40: eval  CrossEntropyLoss | -2.02464414\n",
      "Step     40: eval          Accuracy |  0.03896104\n",
      "\n",
      "Step     45: Ran 5 train steps in 141.91 secs\n",
      "Step     45: train CrossEntropyLoss | -1.89297485\n",
      "Step     45: eval  CrossEntropyLoss | -2.40693331\n",
      "Step     45: eval          Accuracy |  0.04827586\n",
      "\n",
      "Step     50: Ran 5 train steps in 93.11 secs\n",
      "Step     50: train CrossEntropyLoss | -2.21579719\n",
      "Step     50: eval  CrossEntropyLoss | -2.70161653\n",
      "Step     50: eval          Accuracy |  0.04123711\n",
      "\n",
      "Step     55: Ran 5 train steps in 87.23 secs\n",
      "Step     55: train CrossEntropyLoss | -3.10142207\n",
      "Step     55: eval  CrossEntropyLoss | -2.98814321\n",
      "Step     55: eval          Accuracy |  0.02325581\n",
      "\n",
      "Step     60: Ran 5 train steps in 117.65 secs\n",
      "Step     60: train CrossEntropyLoss | -3.70303464\n",
      "Step     60: eval  CrossEntropyLoss | -3.69207287\n",
      "Step     60: eval          Accuracy |  0.02362205\n",
      "\n",
      "Step     65: Ran 5 train steps in 85.03 secs\n",
      "Step     65: train CrossEntropyLoss | -4.66522217\n",
      "Step     65: eval  CrossEntropyLoss | -5.32579231\n",
      "Step     65: eval          Accuracy |  0.04109589\n",
      "\n",
      "Step     70: Ran 5 train steps in 83.43 secs\n",
      "Step     70: train CrossEntropyLoss | -5.37917614\n",
      "Step     70: eval  CrossEntropyLoss | -6.22053337\n",
      "Step     70: eval          Accuracy |  0.04035874\n",
      "\n",
      "Step     75: Ran 5 train steps in 112.93 secs\n",
      "Step     75: train CrossEntropyLoss | -6.08489704\n",
      "Step     75: eval  CrossEntropyLoss | -6.34662056\n",
      "Step     75: eval          Accuracy |  0.02614379\n",
      "\n",
      "Step     80: Ran 5 train steps in 85.72 secs\n",
      "Step     80: train CrossEntropyLoss | -7.08960867\n",
      "Step     80: eval  CrossEntropyLoss | -8.08707333\n",
      "Step     80: eval          Accuracy |  0.04081633\n",
      "\n",
      "Step     85: Ran 5 train steps in 114.23 secs\n",
      "Step     85: train CrossEntropyLoss | -8.49529839\n",
      "Step     85: eval  CrossEntropyLoss | -9.87477970\n",
      "Step     85: eval          Accuracy |  0.04787234\n",
      "\n",
      "Step     90: Ran 5 train steps in 83.79 secs\n",
      "Step     90: train CrossEntropyLoss | -10.04243279\n",
      "Step     90: eval  CrossEntropyLoss | -11.41569805\n",
      "Step     90: eval          Accuracy |  0.04026845\n"
     ]
    }
   ],
   "source": [
    "# Should take around 1.5 minutes\n",
    "# !rm -f model/model.pkl.gz\n",
    "loop = training_loop(TransformerLM, train_batch_stream, eval_batch_stream)\n",
    "loop.load_checkpoint('teste_20_04')\n",
    "loop.run(500)\n",
    "loop.save_checkpoint('teste_20_04')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop"
   ]
  },
  {
   "source": [
    "# Get the model architecture\n",
    "model = TransformerLM(\n",
    "    vocab_size=33300,\n",
    "    d_model=512,\n",
    "    d_ff=2048,\n",
    "    n_layers=6,\n",
    "    n_heads=8,\n",
    "    dropout=0.1,\n",
    "    max_len=4096,\n",
    "    mode='eval',\n",
    "    ff_activation=tl.Relu)\n",
    "\n",
    "# model = TransformerLM(mode='eval')\n",
    "\n",
    "# Load the pre-trained weights\n",
    "model.init_from_file('model/model.pkl.gz', weights_only=True)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C9\n",
    "def next_symbol(cur_output_tokens, model):\n",
    "    \"\"\"Returns the next symbol for a given sentence.\n",
    "\n",
    "    Args:\n",
    "        cur_output_tokens (list): tokenized sentence with EOS and PAD tokens at the end.\n",
    "        model (trax.layers.combinators.Serial): The transformer model.\n",
    "\n",
    "    Returns:\n",
    "        int: tokenized symbol.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    \n",
    "    # current output tokens length\n",
    "    token_length = len(cur_output_tokens)\n",
    "    # calculate the minimum power of 2 big enough to store token_length\n",
    "    # HINT: use np.ceil() and np.log2()\n",
    "    # add 1 to token_length so np.log2() doesn't receive 0 when token_length is 0\n",
    "    padded_length = 2**int(np.ceil(np.log2(token_length + 1)))\n",
    "\n",
    "    # Fill cur_output_tokens with 0's until it reaches padded_length\n",
    "    padded = cur_output_tokens + [0] * (padded_length - token_length)\n",
    "    padded_with_batch = np.array(padded)[None, :] # Don't replace this 'None'! This is a way of setting the batch dim\n",
    "\n",
    "    # model expects a tuple containing two padded tensors (with batch)\n",
    "    output, _ = model((padded_with_batch, padded_with_batch)) \n",
    "    # HINT: output has shape (1, padded_length, vocab_size)\n",
    "    # To get log_probs you need to index output with 0 in the first dim\n",
    "    # token_length in the second dim and all of the entries for the last dim.\n",
    "    log_probs = output[0, token_length, :]\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return int(np.argmax(log_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'10'"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "# Test it out!\n",
    "sentence_test_nxt_symbl = \"I want to fly in the sky tomorrow and it'll be fun.\"\n",
    "detokenize([next_symbol(tokenize(sentence_test_nxt_symbl)+[0], model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C10\n",
    "# Decoding functions.\n",
    "def greedy_decode(input_sentence, model):\n",
    "    \"\"\"Greedy decode function.\n",
    "\n",
    "    Args:\n",
    "        input_sentence (string): a sentence or article.\n",
    "        model (trax.layers.combinators.Serial): Transformer model.\n",
    "\n",
    "    Returns:\n",
    "        string: summary of the input.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    # Use tokenize()\n",
    "    cur_output_tokens = tokenize(input_sentence) + [0]\n",
    "    generated_output = [] \n",
    "    cur_output = 0 \n",
    "    EOS = 1 \n",
    "    \n",
    "    while cur_output != EOS:\n",
    "        # Get next symbol\n",
    "        cur_output = next_symbol(cur_output_tokens, model)\n",
    "        # Append next symbol to original sentence\n",
    "        cur_output_tokens.append(cur_output)\n",
    "        # Append next symbol to generated sentence\n",
    "        generated_output.append(cur_output)\n",
    "        print(detokenize(generated_output))\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return detokenize(generated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "010101010101010101010101010101010101010101010101010101010101010101\n",
      "010101010101010101010101010101010101010101010101010\n",
      "\\101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "01010101010101010101010101010101010101010101010101010\n",
      "\\101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010\n",
      "\\101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "010101010101010101010101010101010101010101010101010101010\n",
      "\\101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "01010101010101010101010101010101010101010101010101010101010\n",
      "\\101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010\n",
      "\\101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "010101010101010101010101010101010101010101010101010101010101010\n",
      "\\101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "01010101010101010101010101010101010101010101010101010101010101010\n",
      "\\101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010\n",
      "\\101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "010101010101010101010101010101010101010101010101010101010101010101010\n",
      "\\101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0\n",
      "\\101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "010\n",
      "\\101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "01010\n",
      "\\101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010\n",
      "\\101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "010101010\n",
      "\\101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "01010101010\n",
      "\\101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010\n",
      "\\101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "010101010101010\n",
      "\\101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "01010101010101010\n",
      "\\101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010\n",
      "\\101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "010101010101010101010\n",
      "\\101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "01010101010101010101010\n",
      "\\101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010\n",
      "\\101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "010101010101010101010101010\n",
      "\\101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "01010101010101010101010101010\n",
      "\\101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010\n",
      "\\101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "010101010101010101010101010101010\n",
      "\\101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "01010101010101010101010101010101010\n",
      "\\101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010\n",
      "\\101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "010101010101010101010101010101010101010\n",
      "\\101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "01010101010101010101010101010101010101010\n",
      "\\101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010\n",
      "\\101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "010101010101010101010101010101010101010101010\n",
      "\\101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "01010101010101010101010101010101010101010101010\n",
      "\\101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010101010101010101010101\n",
      "0101010101010101010101010101010101010101010101010\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-c0121e04f251>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"It was a sunny day when I went to the market to buy some flowers. But I only found roses, not tulips.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgreedy_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-141e779aabc5>\u001b[0m in \u001b[0;36mgreedy_decode\u001b[0;34m(input_sentence, model)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Append next symbol to generated sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mgenerated_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m### END CODE HERE ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-6a2fb88bbdac>\u001b[0m in \u001b[0;36mdetokenize\u001b[0;34m(integers)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdetokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintegers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;34m\"\"\"List of ints to str\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     s = trax.data.detokenize(integers,\n\u001b[0m\u001b[1;32m     16\u001b[0m                              \u001b[0mvocab_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'vocab_dir/'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                              vocab_file='summarize32k.subword.subwords')\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/trax/data/tf_inputs.py\u001b[0m in \u001b[0;36mdetokenize\u001b[0;34m(x, vocab_type, vocab_file, vocab_dir, n_reserved_ids)\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[0mA\u001b[0m \u001b[0mstring\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mde\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtokenized\u001b[0m \u001b[0mversion\u001b[0m \u001b[0mof\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m   \"\"\"\n\u001b[0;32m--> 401\u001b[0;31m   \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m   \u001b[0mx_unreserved\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_reserved_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_unreserved\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/trax/data/tf_inputs.py\u001b[0m in \u001b[0;36m_get_vocab\u001b[0;34m(vocab_type, vocab_file, vocab_dir)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mvocab_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'subword'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtext_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSubwordTextEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mvocab_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'bert'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/trax/data/text_encoder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m    472\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSubwordTextEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/trax/data/text_encoder.py\u001b[0m in \u001b[0;36m_load_from_file\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m    934\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File %s not found\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_from_file_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mstore_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_single_quotes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/trax/data/text_encoder.py\u001b[0m in \u001b[0;36m_load_from_file_object\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    919\u001b[0m     \"\"\"\n\u001b[1;32m    920\u001b[0m     \u001b[0msubtoken_strings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m       \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m       \u001b[0;31m# Some vocab files wrap words in single quotes, but others don't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m     \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;34mr\"\"\"Reads the next line, keeping \\n. At EOF, returns ''.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preread_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_buf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test it out on a sentence!\n",
    "test_sentence = \"It was a sunny day when I went to the market to buy some flowers. But I only found roses, not tulips.\"\n",
    "print(wrapper.fill(test_sentence), '\\n')\n",
    "print(greedy_decode(test_sentence, model))"
   ]
  }
 ]
}