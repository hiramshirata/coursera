{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49",
   "display_name": "Python 3.8.8 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 importing libs\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import textwrap\n",
    "wrapper = textwrap.TextWrapper(width=70)\n",
    "\n",
    "import trax\n",
    "from trax import layers as tl\n",
    "from trax.fastmath import numpy as jnp\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# to print the entire np array\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download train set\n",
    "# train_stream_fn = trax.data.TFDS('scientific_papers',\n",
    "#                                  data_dir='data/',\n",
    "#                                  keys=('article', 'abstract'),\n",
    "#                                  train=True)\n",
    "# eval_stream_fn = trax.data.TFDS('scientific_papers',\n",
    "#                                  data_dir='data/',\n",
    "#                                  keys=('article', 'abstract'),\n",
    "#                                  train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_stream_fn = trax.data.TFDS('cnn_dailymail',\n",
    "                                 data_dir='data/',\n",
    "                                 keys=('article', 'highlights'),\n",
    "                                 train=True)\n",
    "\n",
    "# This should be much faster as the data is downloaded already.\n",
    "eval_stream_fn = trax.data.TFDS('cnn_dailymail',\n",
    "                                data_dir='data/',\n",
    "                                keys=('article', 'highlights'),\n",
    "                                train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocabulary to perform the word to index\n",
    "\n",
    "def tokenize(input_str, EOS=1):\n",
    "    \"\"\"Input str to features dict, ready for inference\"\"\"\n",
    "    # Use the trax.data.tokenize method. It takes streams and returns streams,\n",
    "    # we get around it by making a 1-element stream with `iter`.\n",
    "    inputs =  next(trax.data.tokenize(iter([input_str]),\n",
    "                                      vocab_dir='vocab_dir/',\n",
    "                                      vocab_file='summarize32k.subword.subwords'))\n",
    "    # Mark the end of the sentence with EOS\n",
    "    return list(inputs) + [EOS]\n",
    "\n",
    "def detokenize(integers):\n",
    "    \"\"\"List of ints to str\"\"\"\n",
    "    s = trax.data.detokenize(integers,\n",
    "                             vocab_dir='vocab_dir/',\n",
    "                             vocab_file='summarize32k.subword.subwords')\n",
    "    return wrapper.fill(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special tokens\n",
    "SEP = 0 # Padding or separator token\n",
    "EOS = 1 # End of sentence token\n",
    "\n",
    "# Concatenate tokenized inputs and targets using 0 as separator.\n",
    "def preprocess(stream):\n",
    "    for (article, summary) in stream:\n",
    "        joint = np.array(list(article) + [EOS, SEP] + list(summary) + [EOS])\n",
    "        mask = [0] * (len(list(article)) + 2) + [1] * (len(list(summary)) + 1) # Accounting for EOS and SEP\n",
    "        yield joint, joint, np.array(mask)\n",
    "\n",
    "# You can combine a few data preprocessing steps into a pipeline like this.\n",
    "input_pipeline = trax.data.Serial(\n",
    "    # Tokenizes\n",
    "    trax.data.Tokenize(vocab_dir='vocab_dir/',\n",
    "                       vocab_file='summarize32k.subword.subwords'),\n",
    "    # Uses function defined above\n",
    "    preprocess,\n",
    "    # Filters out examples longer than 4096\n",
    "    trax.data.FilterByLength(4096)\n",
    ")\n",
    "\n",
    "# Apply preprocessing to data streams.\n",
    "train_stream = input_pipeline(train_stream_fn())\n",
    "eval_stream = input_pipeline(eval_stream_fn())\n",
    "\n",
    "train_input, train_target, train_mask = next(train_stream)\n",
    "\n",
    "assert sum((train_input - train_target)**2) == 0  # They are the same in Language Model (LM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, train_target, train_mask = next(train_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Kabul, Afghanistan (CNN) -- Thousands of bottles of alcohol were\\ndestroyed in Kabul this week, in what authorities described as the\\nproduct of a crackdown on illegal smuggling and sales. The bottles\\nwere confiscated over a two-year period in and around the Afghan\\ncapital, according to Kabul police and criminal investigations chief\\nMohammad Zahir. They were taken almost exclusively from \"Afghan\\nsources and not foreigners,\" he said. The illicit items were being\\nstored by Afghan customs officials, who burned the bottles Wednesday\\nafter receiving authorization from the city\\'s attorney general\\'s\\noffice, he added. Alcohol is largely banned in Afghanistan, and its\\nsales and consumption considered a criminal offense for the country\\'s\\n. Muslims, who constitute roughly 99% of the population. Certain areas\\nthat cater to foreigners, however, are permitted to sell it. Zahir\\nsaid that it was in these areas -- mostly international hotels -- that\\nlocal sellers had come into possession of the alcohol. CNN\\'s Matiullah\\nMati contributed to this report .<EOS><pad>Official: Bottles are\\nalmost exclusively from \"Afghan sources\" and not foreigners . Alcohol\\nis largely banned in Afghanistan . Certain areas, however, that cater\\nto foreigners are permitted to sell it .<EOS>'"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "detokenize(train_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 of length < 512. And so on. \n",
    "boundaries =  [128, 256,  512, 1024]\n",
    "batch_sizes = [16,  8,  4,  2,  1]\n",
    "\n",
    "# Create the streams.\n",
    "train_batch_stream = trax.data.BucketByLength(\n",
    "    boundaries, batch_sizes)(train_stream)\n",
    "\n",
    "eval_batch_stream = trax.data.BucketByLength(\n",
    "    boundaries, batch_sizes)(eval_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C1\n",
    "# GRADED FUNCTION: DotProductAttention\n",
    "def DotProductAttention(query, key, value, mask):\n",
    "    \"\"\"Dot product self-attention.\n",
    "    Args:\n",
    "        query (jax.interpreters.xla.DeviceArray): array of query representations with shape (L_q by d)\n",
    "        key (jax.interpreters.xla.DeviceArray): array of key representations with shape (L_k by d)\n",
    "        value (jax.interpreters.xla.DeviceArray): array of value representations with shape (L_k by d) where L_v = L_k\n",
    "        mask (jax.interpreters.xla.DeviceArray): attention-mask, gates attention with shape (L_q by L_k)\n",
    "\n",
    "    Returns:\n",
    "        jax.interpreters.xla.DeviceArray: Self-attention array for q, k, v arrays. (L_q by L_k)\n",
    "    \"\"\"\n",
    "\n",
    "    assert query.shape[-1] == key.shape[-1] == value.shape[-1], \"Embedding dimensions of q, k, v aren't all the same\"\n",
    "\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    # Save depth/dimension of the query embedding for scaling down the dot product\n",
    "    depth = query.shape[-1]\n",
    "\n",
    "    # Calculate scaled query key dot product according to formula above\n",
    "    dots = jnp.matmul(query, jnp.swapaxes(key, -1, -2)) / jnp.sqrt(depth) #rever o conceito do jnp.swapaxes [dots is sqr]\n",
    "    \n",
    "    # Apply the mask\n",
    "    if mask is not None: # The 'None' in this line does not need to be replaced\n",
    "        dots = jnp.where(mask, dots, jnp.full_like(dots, -1e9))\n",
    "    \n",
    "    # Softmax formula implementation\n",
    "    # Use trax.fastmath.logsumexp of dots to avoid underflow by division by large numbers\n",
    "    # Hint: Last axis should be used and keepdims should be True\n",
    "    # Note: softmax = e^(dots - logsumexp(dots)) = E^dots / sumexp(dots)\n",
    "    logsumexp = trax.fastmath.logsumexp(dots, axis=-1, keepdims=True)\n",
    "\n",
    "    # Take exponential of dots minus logsumexp to get softmax\n",
    "    # Use jnp.exp()\n",
    "    dots = jnp.exp(dots - logsumexp)\n",
    "\n",
    "    # Multiply dots by value to get self-attention\n",
    "    # Use jnp.matmul()\n",
    "    attention = jnp.matmul(dots,value)\n",
    "\n",
    "    ## END CODE HERE ###\n",
    "    \n",
    "    return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C2\n",
    "# GRADED FUNCTION: compute_attention_heads_closure\n",
    "def compute_attention_heads_closure(n_heads, d_head):\n",
    "    \"\"\" Function that simulates environment inside CausalAttention function.\n",
    "    Args:\n",
    "        d_head (int):  dimensionality of heads.\n",
    "        n_heads (int): number of attention heads.\n",
    "    Returns:\n",
    "        function: compute_attention_heads function\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_attention_heads(x):\n",
    "        \"\"\" Compute the attention heads.\n",
    "        Args:\n",
    "            x (jax.interpreters.xla.DeviceArray): tensor with shape (batch_size, seqlen, n_heads X d_head).\n",
    "        Returns:\n",
    "            jax.interpreters.xla.DeviceArray: reshaped tensor with shape (batch_size X n_heads, seqlen, d_head).\n",
    "        \"\"\"\n",
    "        ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "        \n",
    "        # Size of the x's batch dimension\n",
    "        batch_size = x.shape[0]\n",
    "        # Length of the sequence\n",
    "        # Should be size of x's first dimension without counting the batch dim\n",
    "        seqlen = x.shape[1]\n",
    "        # Reshape x using jnp.reshape()\n",
    "        # batch_size, seqlen, n_heads*d_head -> batch_size, seqlen, n_heads, d_head\n",
    "        x = jnp.reshape(x, (batch_size, seqlen, n_heads, d_head))\n",
    "        # Transpose x using jnp.transpose()\n",
    "        # batch_size, seqlen, n_heads, d_head -> batch_size, n_heads, seqlen, d_head\n",
    "        # Note that the values within the tuple are the indexes of the dimensions of x and you must rearrange them\n",
    "        x = jnp.transpose(x, (0, 2, 1, 3))\n",
    "        \n",
    "        # Reshape x using jnp.reshape()\n",
    "        # batch_size, n_heads, seqlen, d_head -> batch_size*n_heads, seqlen, d_head\n",
    "        x = jnp.reshape(x, (batch_size*n_heads, seqlen, d_head))\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    return compute_attention_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C3\n",
    "# GRADED FUNCTION: dot_product_self_attention\n",
    "def dot_product_self_attention(q, k, v):\n",
    "    \"\"\" Masked dot product self attention.\n",
    "    Args:\n",
    "        q (jax.interpreters.xla.DeviceArray): queries.\n",
    "        k (jax.interpreters.xla.DeviceArray): keys.\n",
    "        v (jax.interpreters.xla.DeviceArray): values.\n",
    "    Returns:\n",
    "        jax.interpreters.xla.DeviceArray: masked dot product self attention tensor.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    \n",
    "    # Hint: mask size should be equal to L_q. Remember that q has shape (batch_size, L_q, d)\n",
    "    mask_size = q.shape[-2]\n",
    "\n",
    "    # Creates a matrix with ones below the diagonal and 0s above. It should have shape (1, mask_size, mask_size)\n",
    "    # Notice that 1's and 0's get casted to True/False by setting dtype to jnp.bool_\n",
    "    # Use jnp.tril() - Lower triangle of an array and jnp.ones()\n",
    "    mask = jnp.tril(jnp.ones((1, mask_size, mask_size), dtype=jnp.bool_), k=0)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return DotProductAttention(q, k, v, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C4\n",
    "# GRADED FUNCTION: compute_attention_output_closure\n",
    "def compute_attention_output_closure(n_heads, d_head):\n",
    "    \"\"\" Function that simulates environment inside CausalAttention function.\n",
    "    Args:\n",
    "        d_head (int):  dimensionality of heads.\n",
    "        n_heads (int): number of attention heads.\n",
    "    Returns:\n",
    "        function: compute_attention_output function\n",
    "    \"\"\"\n",
    "    \n",
    "    def compute_attention_output(x):\n",
    "        \"\"\" Compute the attention output.\n",
    "        Args:\n",
    "            x (jax.interpreters.xla.DeviceArray): tensor with shape (batch_size X n_heads, seqlen, d_head).\n",
    "        Returns:\n",
    "            jax.interpreters.xla.DeviceArray: reshaped tensor with shape (batch_size, seqlen, n_heads X d_head).\n",
    "        \"\"\"\n",
    "        ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "        \n",
    "        # Length of the sequence\n",
    "        # Should be size of x's first dimension without counting the batch dim\n",
    "        seqlen = x.shape[-2]\n",
    "        # Reshape x using jnp.reshape() to shape (batch_size, n_heads, seqlen, d_head)\n",
    "        x = jnp.reshape(x,(int(x.shape[0]/n_heads),n_heads,seqlen,d_head))\n",
    "        # Transpose x using jnp.transpose() to shape (batch_size, seqlen, n_heads, d_head)\n",
    "        x = jnp.transpose(x,(0,2,1,3))\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Reshape to allow to concatenate the heads\n",
    "        return jnp.reshape(x, (-1, seqlen, n_heads * d_head))\n",
    "    \n",
    "    return compute_attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C5\n",
    "# GRADED FUNCTION: CausalAttention\n",
    "def CausalAttention(d_feature, \n",
    "                    n_heads, \n",
    "                    compute_attention_heads_closure=compute_attention_heads_closure,\n",
    "                    dot_product_self_attention=dot_product_self_attention,\n",
    "                    compute_attention_output_closure=compute_attention_output_closure,\n",
    "                    mode='train'):\n",
    "    \"\"\"Transformer-style multi-headed causal attention.\n",
    "\n",
    "    Args:\n",
    "        d_feature (int):  dimensionality of feature embedding.\n",
    "        n_heads (int): number of attention heads.\n",
    "        compute_attention_heads_closure (function): Closure around compute_attention heads.\n",
    "        dot_product_self_attention (function): dot_product_self_attention function. \n",
    "        compute_attention_output_closure (function): Closure around compute_attention_output. \n",
    "        mode (str): 'train' or 'eval'.\n",
    "\n",
    "    Returns:\n",
    "        trax.layers.combinators.Serial: Multi-headed self-attention model.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert d_feature % n_heads == 0\n",
    "    d_head = d_feature // n_heads\n",
    "\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    \n",
    "    # HINT: The second argument to tl.Fn() is an uncalled function (without the parentheses)\n",
    "    # Since you are dealing with closures you might need to call the outer \n",
    "    # function with the correct parameters to get the actual uncalled function.\n",
    "    ComputeAttentionHeads = tl.Fn('AttnHeads', compute_attention_heads_closure(n_heads,d_head), n_out=1)\n",
    "        \n",
    "\n",
    "    return tl.Serial(\n",
    "        tl.Branch( # creates three towers for one input, takes activations and creates queries keys and values\n",
    "            [tl.Dense(d_feature), ComputeAttentionHeads], # queries\n",
    "            [tl.Dense(d_feature), ComputeAttentionHeads], # keys\n",
    "            [tl.Dense(d_feature), ComputeAttentionHeads], # values\n",
    "        ),\n",
    "        \n",
    "        tl.Fn('DotProductAttn', dot_product_self_attention, n_out=1), # takes QKV\n",
    "        # HINT: The second argument to tl.Fn() is an uncalled function\n",
    "        # Since you are dealing with closures you might need to call the outer \n",
    "        # function with the correct parameters to get the actual uncalled function.\n",
    "        tl.Fn('AttnOutput', compute_attention_output_closure(n_heads,d_head), n_out=1), # to allow for parallel\n",
    "        tl.Dense(d_feature) # Final dense layer\n",
    "    )\n",
    "\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C6\n",
    "# GRADED FUNCTION: DecoderBlock\n",
    "def DecoderBlock(d_model, d_ff, n_heads,\n",
    "                 dropout, mode, ff_activation):\n",
    "    \"\"\"Returns a list of layers that implements a Transformer decoder block.\n",
    "\n",
    "    The input is an activation tensor.\n",
    "\n",
    "    Args:\n",
    "        d_model (int):  depth of embedding.\n",
    "        d_ff (int): depth of feed-forward layer.\n",
    "        n_heads (int): number of attention heads.\n",
    "        dropout (float): dropout rate (how much to drop out).\n",
    "        mode (str): 'train' or 'eval'.\n",
    "        ff_activation (function): the non-linearity in feed-forward layer.\n",
    "\n",
    "    Returns:\n",
    "        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    \n",
    "    # Create masked multi-head attention block using CausalAttention function\n",
    "    causal_attention = CausalAttention( \n",
    "                        d_model,\n",
    "                        n_heads=n_heads,\n",
    "                        mode=mode\n",
    "                        )\n",
    "\n",
    "    # Create feed-forward block (list) with two dense layers with dropout and input normalized\n",
    "    feed_forward = [\n",
    "        # Normalize layer inputs\n",
    "        tl.LayerNorm(),\n",
    "        # Add first feed forward (dense) layer (don't forget to set the correct value for n_units)\n",
    "        tl.Dense(d_ff),\n",
    "        # Add activation function passed in as a parameter (you need to call it!)\n",
    "        ff_activation(), # Generally ReLU\n",
    "        # Add dropout with rate and mode specified (i.e., don't use dropout during evaluation)\n",
    "        tl.Dropout(rate=dropout, mode=mode),\n",
    "        # Add second feed forward layer (don't forget to set the correct value for n_units)\n",
    "        tl.Dense(d_model),\n",
    "        # Add dropout with rate and mode specified (i.e., don't use dropout during evaluation)\n",
    "        tl.Dropout(rate=dropout, mode=mode)\n",
    "    ]\n",
    "\n",
    "    # Add list of two Residual blocks: the attention with normalization and dropout and feed-forward blocks\n",
    "    return [\n",
    "      tl.Residual(\n",
    "          # Normalize layer input\n",
    "          tl.LayerNorm(),\n",
    "          # Add causal attention block previously defined (without parentheses)\n",
    "          causal_attention,\n",
    "          # Add dropout with rate and mode specified\n",
    "          tl.Dropout(rate=dropout, mode=mode)\n",
    "        ),\n",
    "      tl.Residual(\n",
    "          # Add feed forward block (without parentheses)\n",
    "          feed_forward\n",
    "        ),\n",
    "      ]\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C7\n",
    "# GRADED FUNCTION: TransformerLM\n",
    "def TransformerLM(vocab_size=33300,\n",
    "                  d_model=512,\n",
    "                  d_ff=2048,\n",
    "                  n_layers=6,\n",
    "                  n_heads=8,\n",
    "                  dropout=0.1,\n",
    "                  max_len=4096,\n",
    "                  mode='train',\n",
    "                  ff_activation=tl.Relu):\n",
    "    \"\"\"Returns a Transformer language model.\n",
    "\n",
    "    The input to the model is a tensor of tokens. (This model uses only the\n",
    "    decoder part of the overall Transformer.)\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): vocab size.\n",
    "        d_model (int):  depth of embedding.\n",
    "        d_ff (int): depth of feed-forward layer.\n",
    "        n_layers (int): number of decoder layers.\n",
    "        n_heads (int): number of attention heads.\n",
    "        dropout (float): dropout rate (how much to drop out).\n",
    "        max_len (int): maximum symbol length for positional encoding.\n",
    "        mode (str): 'train', 'eval' or 'predict', predict mode is for fast inference.\n",
    "        ff_activation (function): the non-linearity in feed-forward layer.\n",
    "\n",
    "    Returns:\n",
    "        trax.layers.combinators.Serial: A Transformer language model as a layer that maps from a tensor of tokens\n",
    "        to activations over a vocab set.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    \n",
    "    # Embedding inputs and positional encoder\n",
    "    positional_encoder = [ \n",
    "        # Add embedding layer of dimension (vocab_size, d_model)\n",
    "        tl.Embedding(vocab_size, d_model),\n",
    "        # Use dropout with rate and mode specified\n",
    "        tl.Dropout(rate=dropout, mode=mode),\n",
    "        # Add positional encoding layer with maximum input length and mode specified\n",
    "        tl.PositionalEncoding(max_len=max_len, mode=mode)]\n",
    "\n",
    "    # Create stack (list) of decoder blocks with n_layers with necessary parameters\n",
    "    decoder_blocks = [ \n",
    "        DecoderBlock(d_model, d_ff, n_heads,\n",
    "                 dropout, mode, ff_activation) for _ in range(n_layers)]\n",
    "\n",
    "    # Create the complete model as written in the figure\n",
    "    return tl.Serial(\n",
    "        # Use teacher forcing (feed output of previous step to current step)\n",
    "        tl.ShiftRight(mode=mode), # Specify the mode!\n",
    "        # Add positional encoder\n",
    "        positional_encoder,\n",
    "        # Add decoder blocks\n",
    "        decoder_blocks,\n",
    "        # Normalize layer\n",
    "        tl.LayerNorm(),\n",
    "\n",
    "        # Add dense layer of vocab_size (since need to select a word to translate to)\n",
    "        # (a.k.a., logits layer. Note: activation already set by ff_activation)\n",
    "        tl.Dense(vocab_size),\n",
    "        # Get probabilities with Logsoftmax\n",
    "        tl.LogSoftmax()\n",
    "    )\n",
    "\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trax.supervised import training\n",
    "\n",
    "model_path = \"/home/yuguro/Desktop/personal/coursera/text_summarizer/model/\"\n",
    "# UNQ_C8\n",
    "# GRADED FUNCTION: train_model\n",
    "def training_loop(TransformerLM, train_gen, eval_gen, output_dir=model_path):\n",
    "    '''\n",
    "    Input:\n",
    "        TransformerLM (trax.layers.combinators.Serial): The model you are building.\n",
    "        train_gen (generator): Training stream of data.\n",
    "        eval_gen (generator): Evaluation stream of data.\n",
    "        output_dir (str): folder to save your file.\n",
    "        \n",
    "    Returns:\n",
    "        trax.supervised.training.Loop: Training loop.\n",
    "    '''\n",
    "    output_dir = os.path.expanduser(output_dir)  # trainer is an object\n",
    "    lr_schedule = trax.lr.warmup_and_rsqrt_decay(n_warmup_steps=10000, max_value=0.01)\n",
    "\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    train_task = training.TrainTask( \n",
    "      labeled_data=train_gen, # The training generator\n",
    "      loss_layer=tl.CrossEntropyLoss(), # Loss function \n",
    "      optimizer=trax.optimizers.Adam(0.01), # Optimizer (Don't forget to set LR to 0.01)\n",
    "      lr_schedule=lr_schedule,\n",
    "      n_steps_per_checkpoint=10\n",
    "    )\n",
    "\n",
    "    eval_task = training.EvalTask( \n",
    "      labeled_data=eval_gen, # The evaluation generator\n",
    "      metrics=[tl.CrossEntropyLoss(), tl.Accuracy()] # CrossEntropyLoss and Accuracy\n",
    "    )\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    loop = training.Loop(TransformerLM(d_model=4,\n",
    "                                       d_ff=16,\n",
    "                                       n_layers=1,\n",
    "                                       n_heads=2,\n",
    "                                       mode='train'),\n",
    "                         train_task,\n",
    "                         eval_tasks=[eval_task],\n",
    "                         output_dir=output_dir)\n",
    "    \n",
    "    return loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "8853836\n",
      "Step    150: eval          Accuracy |  0.03587444\n",
      "\n",
      "Step    160: Ran 10 train steps in 19.59 secs\n",
      "Step    160: train CrossEntropyLoss |  10.39057827\n",
      "Step    160: eval  CrossEntropyLoss |  10.39022636\n",
      "Step    160: eval          Accuracy |  0.00653595\n",
      "\n",
      "Step    170: Ran 10 train steps in 22.83 secs\n",
      "Step    170: train CrossEntropyLoss |  10.38604546\n",
      "Step    170: eval  CrossEntropyLoss |  10.38398361\n",
      "Step    170: eval          Accuracy |  0.01020408\n",
      "\n",
      "Step    180: Ran 10 train steps in 22.48 secs\n",
      "Step    180: train CrossEntropyLoss |  10.38374424\n",
      "Step    180: eval  CrossEntropyLoss |  10.37525368\n",
      "Step    180: eval          Accuracy |  0.05319149\n",
      "\n",
      "Step    190: Ran 10 train steps in 13.03 secs\n",
      "Step    190: train CrossEntropyLoss |  10.37728310\n",
      "Step    190: eval  CrossEntropyLoss |  10.36801434\n",
      "Step    190: eval          Accuracy |  0.04026845\n",
      "\n",
      "Step    200: Ran 10 train steps in 22.27 secs\n",
      "Step    200: train CrossEntropyLoss |  10.37132263\n",
      "Step    200: eval  CrossEntropyLoss |  10.36741543\n",
      "Step    200: eval          Accuracy |  0.04651163\n",
      "\n",
      "Step    210: Ran 10 train steps in 22.59 secs\n",
      "Step    210: train CrossEntropyLoss |  10.36460304\n",
      "Step    210: eval  CrossEntropyLoss |  10.36829376\n",
      "Step    210: eval          Accuracy |  0.03418804\n",
      "\n",
      "Step    220: Ran 10 train steps in 22.18 secs\n",
      "Step    220: train CrossEntropyLoss |  10.36176109\n",
      "Step    220: eval  CrossEntropyLoss |  10.35470676\n",
      "Step    220: eval          Accuracy |  0.02542373\n",
      "\n",
      "Step    230: Ran 10 train steps in 26.09 secs\n",
      "Step    230: train CrossEntropyLoss |  10.34877586\n",
      "Step    230: eval  CrossEntropyLoss |  10.33890724\n",
      "Step    230: eval          Accuracy |  0.04511278\n",
      "\n",
      "Step    240: Ran 10 train steps in 22.60 secs\n",
      "Step    240: train CrossEntropyLoss |  10.33850670\n",
      "Step    240: eval  CrossEntropyLoss |  10.33980179\n",
      "Step    240: eval          Accuracy |  0.03428571\n",
      "\n",
      "Step    250: Ran 10 train steps in 19.61 secs\n",
      "Step    250: train CrossEntropyLoss |  10.32916355\n",
      "Step    250: eval  CrossEntropyLoss |  10.33267307\n",
      "Step    250: eval          Accuracy |  0.05042017\n",
      "\n",
      "Step    260: Ran 10 train steps in 22.91 secs\n",
      "Step    260: train CrossEntropyLoss |  10.32424164\n",
      "Step    260: eval  CrossEntropyLoss |  10.32339001\n",
      "Step    260: eval          Accuracy |  0.02290076\n",
      "\n",
      "Step    270: Ran 10 train steps in 21.72 secs\n",
      "Step    270: train CrossEntropyLoss |  10.30970287\n",
      "Step    270: eval  CrossEntropyLoss |  10.30109119\n",
      "Step    270: eval          Accuracy |  0.03301887\n",
      "\n",
      "Step    280: Ran 10 train steps in 26.14 secs\n",
      "Step    280: train CrossEntropyLoss |  10.29857063\n",
      "Step    280: eval  CrossEntropyLoss |  10.29274464\n",
      "Step    280: eval          Accuracy |  0.04697987\n",
      "\n",
      "Step    290: Ran 10 train steps in 22.92 secs\n",
      "Step    290: train CrossEntropyLoss |  10.28605461\n",
      "Step    290: eval  CrossEntropyLoss |  10.27085686\n",
      "Step    290: eval          Accuracy |  0.01754386\n",
      "\n",
      "Step    300: Ran 10 train steps in 16.11 secs\n",
      "Step    300: train CrossEntropyLoss |  10.27105522\n",
      "Step    300: eval  CrossEntropyLoss |  10.26772594\n",
      "Step    300: eval          Accuracy |  0.02352941\n",
      "\n",
      "Step    310: Ran 10 train steps in 16.38 secs\n",
      "Step    310: train CrossEntropyLoss |  10.25581074\n",
      "Step    310: eval  CrossEntropyLoss |  10.25131607\n",
      "Step    310: eval          Accuracy |  0.04444445\n",
      "\n",
      "Step    320: Ran 10 train steps in 25.98 secs\n",
      "Step    320: train CrossEntropyLoss |  10.23700905\n",
      "Step    320: eval  CrossEntropyLoss |  10.22696972\n",
      "Step    320: eval          Accuracy |  0.05000000\n",
      "\n",
      "Step    330: Ran 10 train steps in 19.33 secs\n",
      "Step    330: train CrossEntropyLoss |  10.22706127\n",
      "Step    330: eval  CrossEntropyLoss |  10.22135544\n",
      "Step    330: eval          Accuracy |  0.03144654\n",
      "\n",
      "Step    340: Ran 10 train steps in 19.09 secs\n",
      "Step    340: train CrossEntropyLoss |  10.20540619\n",
      "Step    340: eval  CrossEntropyLoss |  10.18279552\n",
      "Step    340: eval          Accuracy |  0.02631579\n",
      "\n",
      "Step    350: Ran 10 train steps in 16.11 secs\n",
      "Step    350: train CrossEntropyLoss |  10.18811512\n",
      "Step    350: eval  CrossEntropyLoss |  10.15758228\n",
      "Step    350: eval          Accuracy |  0.04761905\n",
      "\n",
      "Step    360: Ran 10 train steps in 22.82 secs\n",
      "Step    360: train CrossEntropyLoss |  10.16545677\n",
      "Step    360: eval  CrossEntropyLoss |  10.12422180\n",
      "Step    360: eval          Accuracy |  0.04651163\n",
      "\n",
      "Step    370: Ran 10 train steps in 19.36 secs\n",
      "Step    370: train CrossEntropyLoss |  10.14247227\n",
      "Step    370: eval  CrossEntropyLoss |  10.14256382\n",
      "Step    370: eval          Accuracy |  0.03296703\n",
      "\n",
      "Step    380: Ran 10 train steps in 19.23 secs\n",
      "Step    380: train CrossEntropyLoss |  10.12205315\n",
      "Step    380: eval  CrossEntropyLoss |  10.15127945\n",
      "Step    380: eval          Accuracy |  0.02000000\n",
      "\n",
      "Step    390: Ran 10 train steps in 17.01 secs\n",
      "Step    390: train CrossEntropyLoss |  10.10355949\n",
      "Step    390: eval  CrossEntropyLoss |  10.09922600\n",
      "Step    390: eval          Accuracy |  0.02645503\n",
      "\n",
      "Step    400: Ran 10 train steps in 6.59 secs\n",
      "Step    400: train CrossEntropyLoss |  10.07141685\n",
      "Step    400: eval  CrossEntropyLoss |  10.07654953\n",
      "Step    400: eval          Accuracy |  0.04022989\n",
      "\n",
      "Step    410: Ran 10 train steps in 19.14 secs\n",
      "Step    410: train CrossEntropyLoss |  10.07164192\n",
      "Step    410: eval  CrossEntropyLoss |  10.06080818\n",
      "Step    410: eval          Accuracy |  0.03825137\n",
      "\n",
      "Step    420: Ran 10 train steps in 19.33 secs\n",
      "Step    420: train CrossEntropyLoss |  10.03487492\n",
      "Step    420: eval  CrossEntropyLoss |  10.09256363\n",
      "Step    420: eval          Accuracy |  0.03086420\n",
      "\n",
      "Step    430: Ran 10 train steps in 16.49 secs\n",
      "Step    430: train CrossEntropyLoss |  9.99648285\n",
      "Step    430: eval  CrossEntropyLoss |  9.92461109\n",
      "Step    430: eval          Accuracy |  0.03797468\n",
      "\n",
      "Step    440: Ran 10 train steps in 19.55 secs\n",
      "Step    440: train CrossEntropyLoss |  9.98153591\n",
      "Step    440: eval  CrossEntropyLoss |  9.95276928\n",
      "Step    440: eval          Accuracy |  0.02985075\n",
      "\n",
      "Step    450: Ran 10 train steps in 21.62 secs\n",
      "Step    450: train CrossEntropyLoss |  9.95520210\n",
      "Step    450: eval  CrossEntropyLoss |  9.90003681\n",
      "Step    450: eval          Accuracy |  0.03205128\n",
      "\n",
      "Step    460: Ran 10 train steps in 22.31 secs\n",
      "Step    460: train CrossEntropyLoss |  9.92879677\n",
      "Step    460: eval  CrossEntropyLoss |  9.88135815\n",
      "Step    460: eval          Accuracy |  0.03472222\n",
      "\n",
      "Step    470: Ran 10 train steps in 16.48 secs\n",
      "Step    470: train CrossEntropyLoss |  9.88903618\n",
      "Step    470: eval  CrossEntropyLoss |  9.85866547\n",
      "Step    470: eval          Accuracy |  0.04065040\n",
      "\n",
      "Step    480: Ran 10 train steps in 19.59 secs\n",
      "Step    480: train CrossEntropyLoss |  9.85408592\n",
      "Step    480: eval  CrossEntropyLoss |  9.85171700\n",
      "Step    480: eval          Accuracy |  0.03333334\n",
      "\n",
      "Step    490: Ran 10 train steps in 16.02 secs\n",
      "Step    490: train CrossEntropyLoss |  9.83624077\n",
      "Step    490: eval  CrossEntropyLoss |  9.79613400\n",
      "Step    490: eval          Accuracy |  0.01041667\n",
      "\n",
      "Step    500: Ran 10 train steps in 19.51 secs\n",
      "Step    500: train CrossEntropyLoss |  9.80524349\n",
      "Step    500: eval  CrossEntropyLoss |  9.72806454\n",
      "Step    500: eval          Accuracy |  0.03401361\n",
      "\n",
      "Step    510: Ran 10 train steps in 19.39 secs\n",
      "Step    510: train CrossEntropyLoss |  9.74606514\n",
      "Step    510: eval  CrossEntropyLoss |  9.74510860\n",
      "Step    510: eval          Accuracy |  0.02941176\n",
      "\n",
      "Step    520: Ran 10 train steps in 19.99 secs\n",
      "Step    520: train CrossEntropyLoss |  9.74092102\n",
      "Step    520: eval  CrossEntropyLoss |  9.74418068\n",
      "Step    520: eval          Accuracy |  0.02127660\n",
      "\n",
      "Step    530: Ran 10 train steps in 22.92 secs\n",
      "Step    530: train CrossEntropyLoss |  9.66376591\n",
      "Step    530: eval  CrossEntropyLoss |  9.73264599\n",
      "Step    530: eval          Accuracy |  0.03846154\n",
      "\n",
      "Step    540: Ran 10 train steps in 21.90 secs\n",
      "Step    540: train CrossEntropyLoss |  9.64780426\n",
      "Step    540: eval  CrossEntropyLoss |  9.58440971\n",
      "Step    540: eval          Accuracy |  0.03921569\n",
      "\n",
      "Step    550: Ran 10 train steps in 16.37 secs\n",
      "Step    550: train CrossEntropyLoss |  9.58207893\n",
      "Step    550: eval  CrossEntropyLoss |  9.59193134\n",
      "Step    550: eval          Accuracy |  0.02890173\n",
      "\n",
      "Step    560: Ran 10 train steps in 19.13 secs\n",
      "Step    560: train CrossEntropyLoss |  9.57679653\n",
      "Step    560: eval  CrossEntropyLoss |  9.59681988\n",
      "Step    560: eval          Accuracy |  0.04188482\n",
      "\n",
      "Step    570: Ran 10 train steps in 22.58 secs\n",
      "Step    570: train CrossEntropyLoss |  9.51725578\n",
      "Step    570: eval  CrossEntropyLoss |  9.47878647\n",
      "Step    570: eval          Accuracy |  0.03243243\n",
      "\n",
      "Step    580: Ran 10 train steps in 19.69 secs\n",
      "Step    580: train CrossEntropyLoss |  9.45548725\n",
      "Step    580: eval  CrossEntropyLoss |  9.49609947\n",
      "Step    580: eval          Accuracy |  0.02068966\n",
      "\n",
      "Step    590: Ran 10 train steps in 6.76 secs\n",
      "Step    590: train CrossEntropyLoss |  9.44504452\n",
      "Step    590: eval  CrossEntropyLoss |  9.34003735\n",
      "Step    590: eval          Accuracy |  0.03921569\n",
      "\n",
      "Step    600: Ran 10 train steps in 3.70 secs\n",
      "Step    600: train CrossEntropyLoss |  9.37618065\n",
      "Step    600: eval  CrossEntropyLoss |  9.38875008\n",
      "Step    600: eval          Accuracy |  0.03448276\n",
      "\n",
      "Step    610: Ran 10 train steps in 16.75 secs\n",
      "Step    610: train CrossEntropyLoss |  9.33477402\n",
      "Step    610: eval  CrossEntropyLoss |  9.28601742\n",
      "Step    610: eval          Accuracy |  0.02197802\n",
      "\n",
      "Step    620: Ran 10 train steps in 15.94 secs\n",
      "Step    620: train CrossEntropyLoss |  9.28609276\n",
      "Step    620: eval  CrossEntropyLoss |  9.21631432\n",
      "Step    620: eval          Accuracy |  0.04166667\n",
      "\n",
      "Step    630: Ran 10 train steps in 12.80 secs\n",
      "Step    630: train CrossEntropyLoss |  9.25714016\n",
      "Step    630: eval  CrossEntropyLoss |  9.22662067\n",
      "Step    630: eval          Accuracy |  0.02542373\n",
      "\n",
      "Step    640: Ran 10 train steps in 13.46 secs\n",
      "Step    640: train CrossEntropyLoss |  9.22615147\n",
      "Step    640: eval  CrossEntropyLoss |  9.19865799\n",
      "Step    640: eval          Accuracy |  0.02307692\n",
      "\n",
      "Step    650: Ran 10 train steps in 12.74 secs\n",
      "Step    650: train CrossEntropyLoss |  9.12313652\n",
      "Step    650: eval  CrossEntropyLoss |  9.07160091\n",
      "Step    650: eval          Accuracy |  0.03389831\n",
      "\n",
      "Step    660: Ran 10 train steps in 14.15 secs\n",
      "Step    660: train CrossEntropyLoss |  9.13201523\n",
      "Step    660: eval  CrossEntropyLoss |  9.04347897\n",
      "Step    660: eval          Accuracy |  0.02604167\n",
      "\n",
      "Step    670: Ran 10 train steps in 22.04 secs\n",
      "Step    670: train CrossEntropyLoss |  9.07182503\n",
      "Step    670: eval  CrossEntropyLoss |  9.13912296\n",
      "Step    670: eval          Accuracy |  0.03278688\n",
      "\n",
      "Step    680: Ran 10 train steps in 16.49 secs\n",
      "Step    680: train CrossEntropyLoss |  9.03079510\n",
      "Step    680: eval  CrossEntropyLoss |  8.90639591\n",
      "Step    680: eval          Accuracy |  0.02777778\n",
      "\n",
      "Step    690: Ran 10 train steps in 19.12 secs\n",
      "Step    690: train CrossEntropyLoss |  8.97334576\n",
      "Step    690: eval  CrossEntropyLoss |  8.91255093\n",
      "Step    690: eval          Accuracy |  0.03571429\n",
      "\n",
      "Step    700: Ran 10 train steps in 13.25 secs\n",
      "Step    700: train CrossEntropyLoss |  8.89912224\n",
      "Step    700: eval  CrossEntropyLoss |  8.82345676\n",
      "Step    700: eval          Accuracy |  0.03381642\n",
      "\n",
      "Step    710: Ran 10 train steps in 10.10 secs\n",
      "Step    710: train CrossEntropyLoss |  8.83714867\n",
      "Step    710: eval  CrossEntropyLoss |  8.86987972\n",
      "Step    710: eval          Accuracy |  0.04081633\n",
      "\n",
      "Step    720: Ran 10 train steps in 19.62 secs\n",
      "Step    720: train CrossEntropyLoss |  8.82586098\n",
      "Step    720: eval  CrossEntropyLoss |  8.68377876\n",
      "Step    720: eval          Accuracy |  0.02173913\n",
      "\n",
      "Step    730: Ran 10 train steps in 16.32 secs\n",
      "Step    730: train CrossEntropyLoss |  8.72459126\n",
      "Step    730: eval  CrossEntropyLoss |  8.63050842\n",
      "Step    730: eval          Accuracy |  0.02580645\n",
      "\n",
      "Step    740: Ran 10 train steps in 29.23 secs\n",
      "Step    740: train CrossEntropyLoss |  8.69192219\n",
      "Step    740: eval  CrossEntropyLoss |  8.58480167\n",
      "Step    740: eval          Accuracy |  0.02083333\n",
      "\n",
      "Step    750: Ran 10 train steps in 19.61 secs\n",
      "Step    750: train CrossEntropyLoss |  8.69036865\n",
      "Step    750: eval  CrossEntropyLoss |  8.72593784\n",
      "Step    750: eval          Accuracy |  0.03030303\n",
      "\n",
      "Step    760: Ran 10 train steps in 13.11 secs\n",
      "Step    760: train CrossEntropyLoss |  8.55719185\n",
      "Step    760: eval  CrossEntropyLoss |  8.65477657\n",
      "Step    760: eval          Accuracy |  0.03619910\n",
      "\n",
      "Step    770: Ran 10 train steps in 19.67 secs\n",
      "Step    770: train CrossEntropyLoss |  8.48968887\n",
      "Step    770: eval  CrossEntropyLoss |  8.41132641\n",
      "Step    770: eval          Accuracy |  0.02564103\n",
      "\n",
      "Step    780: Ran 10 train steps in 7.69 secs\n",
      "Step    780: train CrossEntropyLoss |  8.33839226\n",
      "Step    780: eval  CrossEntropyLoss |  8.20738029\n",
      "Step    780: eval          Accuracy |  0.04188482\n",
      "\n",
      "Step    790: Ran 10 train steps in 19.32 secs\n",
      "Step    790: train CrossEntropyLoss |  8.34972954\n",
      "Step    790: eval  CrossEntropyLoss |  8.42610550\n",
      "Step    790: eval          Accuracy |  0.03092784\n",
      "\n",
      "Step    800: Ran 10 train steps in 19.51 secs\n",
      "Step    800: train CrossEntropyLoss |  8.29638863\n",
      "Step    800: eval  CrossEntropyLoss |  8.12814713\n",
      "Step    800: eval          Accuracy |  0.03521127\n",
      "\n",
      "Step    810: Ran 10 train steps in 24.86 secs\n",
      "Step    810: train CrossEntropyLoss |  8.21997261\n",
      "Step    810: eval  CrossEntropyLoss |  8.23721886\n",
      "Step    810: eval          Accuracy |  0.04210526\n",
      "\n",
      "Step    820: Ran 10 train steps in 16.09 secs\n",
      "Step    820: train CrossEntropyLoss |  8.15236473\n",
      "Step    820: eval  CrossEntropyLoss |  8.09200859\n",
      "Step    820: eval          Accuracy |  0.03278688\n",
      "\n",
      "Step    830: Ran 10 train steps in 13.15 secs\n",
      "Step    830: train CrossEntropyLoss |  8.11994076\n",
      "Step    830: eval  CrossEntropyLoss |  7.90452576\n",
      "Step    830: eval          Accuracy |  0.03409091\n",
      "\n",
      "Step    840: Ran 10 train steps in 13.07 secs\n",
      "Step    840: train CrossEntropyLoss |  8.07432365\n",
      "Step    840: eval  CrossEntropyLoss |  7.92687464\n",
      "Step    840: eval          Accuracy |  0.02777778\n",
      "\n",
      "Step    850: Ran 10 train steps in 19.01 secs\n",
      "Step    850: train CrossEntropyLoss |  7.95610142\n",
      "Step    850: eval  CrossEntropyLoss |  8.00301933\n",
      "Step    850: eval          Accuracy |  0.04705882\n",
      "\n",
      "Step    860: Ran 10 train steps in 13.26 secs\n",
      "Step    860: train CrossEntropyLoss |  7.98228455\n",
      "Step    860: eval  CrossEntropyLoss |  7.90738344\n",
      "Step    860: eval          Accuracy |  0.03361345\n",
      "\n",
      "Step    870: Ran 10 train steps in 13.26 secs\n",
      "Step    870: train CrossEntropyLoss |  7.91935873\n",
      "Step    870: eval  CrossEntropyLoss |  7.77194262\n",
      "Step    870: eval          Accuracy |  0.04629629\n",
      "\n",
      "Step    880: Ran 10 train steps in 16.25 secs\n",
      "Step    880: train CrossEntropyLoss |  7.82956028\n",
      "Step    880: eval  CrossEntropyLoss |  7.82463789\n",
      "Step    880: eval          Accuracy |  0.04641350\n",
      "\n",
      "Step    890: Ran 10 train steps in 10.02 secs\n",
      "Step    890: train CrossEntropyLoss |  7.86478806\n",
      "Step    890: eval  CrossEntropyLoss |  7.83567095\n",
      "Step    890: eval          Accuracy |  0.04081633\n",
      "\n",
      "Step    900: Ran 10 train steps in 3.19 secs\n",
      "Step    900: train CrossEntropyLoss |  7.72352839\n",
      "Step    900: eval  CrossEntropyLoss |  7.30240297\n",
      "Step    900: eval          Accuracy |  0.04522613\n",
      "\n",
      "Step    910: Ran 10 train steps in 16.55 secs\n",
      "Step    910: train CrossEntropyLoss |  7.74534464\n",
      "Step    910: eval  CrossEntropyLoss |  7.69595861\n",
      "Step    910: eval          Accuracy |  0.03030303\n",
      "\n",
      "Step    920: Ran 10 train steps in 6.40 secs\n",
      "Step    920: train CrossEntropyLoss |  7.57623672\n",
      "Step    920: eval  CrossEntropyLoss |  7.42149639\n",
      "Step    920: eval          Accuracy |  0.04545455\n",
      "\n",
      "Step    930: Ran 10 train steps in 12.78 secs\n",
      "Step    930: train CrossEntropyLoss |  7.53608084\n",
      "Step    930: eval  CrossEntropyLoss |  7.55047989\n",
      "Step    930: eval          Accuracy |  0.03267974\n",
      "\n",
      "Step    940: Ran 10 train steps in 13.54 secs\n",
      "Step    940: train CrossEntropyLoss |  7.50245285\n",
      "Step    940: eval  CrossEntropyLoss |  7.77379704\n",
      "Step    940: eval          Accuracy |  0.02469136\n",
      "\n",
      "Step    950: Ran 10 train steps in 9.22 secs\n",
      "Step    950: train CrossEntropyLoss |  7.45985270\n",
      "Step    950: eval  CrossEntropyLoss |  7.33941317\n",
      "Step    950: eval          Accuracy |  0.04210526\n",
      "\n",
      "Step    960: Ran 10 train steps in 19.66 secs\n",
      "Step    960: train CrossEntropyLoss |  7.39486933\n",
      "Step    960: eval  CrossEntropyLoss |  7.25556612\n",
      "Step    960: eval          Accuracy |  0.04629629\n",
      "\n",
      "Step    970: Ran 10 train steps in 12.37 secs\n",
      "Step    970: train CrossEntropyLoss |  7.43737793\n",
      "Step    970: eval  CrossEntropyLoss |  7.16630459\n",
      "Step    970: eval          Accuracy |  0.03960396\n",
      "\n",
      "Step    980: Ran 10 train steps in 12.89 secs\n",
      "Step    980: train CrossEntropyLoss |  7.45394373\n",
      "Step    980: eval  CrossEntropyLoss |  7.14513636\n",
      "Step    980: eval          Accuracy |  0.03883495\n",
      "\n",
      "Step    990: Ran 10 train steps in 16.19 secs\n",
      "Step    990: train CrossEntropyLoss |  7.41908121\n",
      "Step    990: eval  CrossEntropyLoss |  7.39228535\n",
      "Step    990: eval          Accuracy |  0.03623188\n",
      "\n",
      "Step   1000: Ran 10 train steps in 12.78 secs\n",
      "Step   1000: train CrossEntropyLoss |  7.30583429\n",
      "Step   1000: eval  CrossEntropyLoss |  7.37023640\n",
      "Step   1000: eval          Accuracy |  0.03333334\n",
      "\n",
      "Step   1010: Ran 10 train steps in 20.30 secs\n",
      "Step   1010: train CrossEntropyLoss |  7.48878384\n",
      "Step   1010: eval  CrossEntropyLoss |  7.23290634\n",
      "Step   1010: eval          Accuracy |  0.03205128\n",
      "\n",
      "Step   1020: Ran 10 train steps in 18.85 secs\n",
      "Step   1020: train CrossEntropyLoss |  7.31859732\n",
      "Step   1020: eval  CrossEntropyLoss |  7.16394377\n",
      "Step   1020: eval          Accuracy |  0.03184713\n",
      "\n",
      "Step   1030: Ran 10 train steps in 26.46 secs\n",
      "Step   1030: train CrossEntropyLoss |  7.40167332\n",
      "Step   1030: eval  CrossEntropyLoss |  7.14405966\n",
      "Step   1030: eval          Accuracy |  0.05517241\n",
      "\n",
      "Step   1040: Ran 10 train steps in 12.93 secs\n",
      "Step   1040: train CrossEntropyLoss |  7.44121552\n",
      "Step   1040: eval  CrossEntropyLoss |  7.64501619\n",
      "Step   1040: eval          Accuracy |  0.00000000\n",
      "\n",
      "Step   1050: Ran 10 train steps in 9.54 secs\n",
      "Step   1050: train CrossEntropyLoss |  7.35678339\n",
      "Step   1050: eval  CrossEntropyLoss |  7.42543554\n",
      "Step   1050: eval          Accuracy |  0.05617978\n",
      "\n",
      "Step   1060: Ran 10 train steps in 15.65 secs\n",
      "Step   1060: train CrossEntropyLoss |  7.29611683\n",
      "Step   1060: eval  CrossEntropyLoss |  7.55988359\n",
      "Step   1060: eval          Accuracy |  0.02702703\n",
      "\n",
      "Step   1070: Ran 10 train steps in 9.82 secs\n",
      "Step   1070: train CrossEntropyLoss |  7.46193027\n",
      "Step   1070: eval  CrossEntropyLoss |  7.59014463\n",
      "Step   1070: eval          Accuracy |  0.02762431\n",
      "\n",
      "Step   1080: Ran 10 train steps in 6.63 secs\n",
      "Step   1080: train CrossEntropyLoss |  7.34016323\n",
      "Step   1080: eval  CrossEntropyLoss |  7.09783125\n",
      "Step   1080: eval          Accuracy |  0.03636364\n",
      "\n",
      "Step   1090: Ran 10 train steps in 11.90 secs\n",
      "Step   1090: train CrossEntropyLoss |  7.47127819\n",
      "Step   1090: eval  CrossEntropyLoss |  7.18527651\n",
      "Step   1090: eval          Accuracy |  0.08771930\n",
      "\n",
      "Step   1100: Ran 10 train steps in 20.38 secs\n",
      "Step   1100: train CrossEntropyLoss |  7.35536671\n",
      "Step   1100: eval  CrossEntropyLoss |  7.40014601\n",
      "Step   1100: eval          Accuracy |  0.07070707\n",
      "\n",
      "Step   1110: Ran 10 train steps in 22.61 secs\n",
      "Step   1110: train CrossEntropyLoss |  7.28580475\n",
      "Step   1110: eval  CrossEntropyLoss |  6.94578171\n",
      "Step   1110: eval          Accuracy |  0.05415162\n",
      "\n",
      "Step   1120: Ran 10 train steps in 17.61 secs\n",
      "Step   1120: train CrossEntropyLoss |  7.34622669\n",
      "Step   1120: eval  CrossEntropyLoss |  7.54465628\n",
      "Step   1120: eval          Accuracy |  0.03773585\n",
      "\n",
      "Step   1130: Ran 10 train steps in 15.50 secs\n",
      "Step   1130: train CrossEntropyLoss |  7.24741077\n",
      "Step   1130: eval  CrossEntropyLoss |  7.51459980\n",
      "Step   1130: eval          Accuracy |  0.02162162\n",
      "\n",
      "Step   1140: Ran 10 train steps in 23.07 secs\n",
      "Step   1140: train CrossEntropyLoss |  7.36788940\n",
      "Step   1140: eval  CrossEntropyLoss |  7.39969349\n",
      "Step   1140: eval          Accuracy |  0.03703704\n",
      "\n",
      "Step   1150: Ran 10 train steps in 10.68 secs\n",
      "Step   1150: train CrossEntropyLoss |  7.30605459\n",
      "Step   1150: eval  CrossEntropyLoss |  7.04209280\n",
      "Step   1150: eval          Accuracy |  0.04411765\n",
      "\n",
      "Step   1160: Ran 10 train steps in 547.33 secs\n",
      "Step   1160: train CrossEntropyLoss |  7.18850946\n",
      "Step   1160: eval  CrossEntropyLoss |  6.88473320\n",
      "Step   1160: eval          Accuracy |  0.04597701\n"
     ]
    }
   ],
   "source": [
    "# Should take around 1.5 minutes\n",
    "!rm -f model/model.pkl.gz\n",
    "loop = training_loop(TransformerLM, train_batch_stream, eval_batch_stream)\n",
    "loop.run(10000)"
   ]
  }
 ]
}