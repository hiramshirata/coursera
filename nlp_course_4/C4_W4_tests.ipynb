{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "5b05dd675de065a8716e68f1e56952380c3ee1000fc3bff67b6b7ad1bcaa0b69"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 sorting buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n",
      "WARNING:root:Argument blacklist is deprecated. Please use denylist.\n"
     ]
    }
   ],
   "source": [
    "import trax\n",
    "import numpy as np\n",
    "from trax import layers as tl\n",
    "from trax import fastmath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_n_hashes = 2\n",
    "t_n_buckets = 4\n",
    "t_n_seq = t_seqlen = 8\n",
    "t_n_q = 3\n",
    "n_v = 5\n",
    "\n",
    "t_q = (np.array([(j % t_n_buckets) for j in range(t_n_seq)]) * np.ones((t_n_q, 1))).T\n",
    "t_v = np.ones((t_n_seq, n_v))\n",
    "t_buckets = np.array(\n",
    "    [\n",
    "        (j % t_n_buckets) + t_n_buckets * i\n",
    "        for i in range(t_n_hashes)\n",
    "        for j in range(t_n_seq)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_buckets(buckets, q, v, n_buckets, n_hashes, seqlen, verbose=False):\n",
    "    \"\"\" \n",
    "  Args:\n",
    "    buckets: tensor of at least 2 dimension, \n",
    "    n_buckets: number of buckets in each hash table\n",
    "    n_hashes: the number of hash tables    \n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"---sort_buckets--\")\n",
    "    ## Step 1\n",
    "    ticker = np.arange(n_hashes*seqlen)\n",
    "    if verbose:\n",
    "        print(\"ticker\", ticker.shape, ticker)\n",
    "    ## Step 2\n",
    "    buckets_and_t = seqlen * buckets + (ticker % seqlen)  # provided\n",
    "    if verbose:\n",
    "        print(\"buckets_and_t\", buckets_and_t.shape, buckets_and_t)\n",
    "\n",
    "    # Hash-based sort (\"s\" at the start of variable names means \"sorted\")\n",
    "    # Step 3\n",
    "    sbuckets_and_t, sticker = fastmath.sort_key_val(buckets_and_t, ticker, dimension=-1)\n",
    "    if verbose:\n",
    "        print(\"sbuckets_and_t\", sbuckets_and_t.shape, sbuckets_and_t)\n",
    "    if verbose:\n",
    "        print(\"sticker\", sticker.shape, sticker)\n",
    "    # Step 4\n",
    "    _, undo_sort = fastmath.sort_key_val(sticker, ticker, dimension=-1)\n",
    "    if verbose:\n",
    "        print(\"undo_sort\", undo_sort.shape, undo_sort)\n",
    "\n",
    "    # Step 5\n",
    "    st = sticker % seqlen  # provided\n",
    "    sq = np.take(q, st, axis = 0)\n",
    "    sv = np.take(v, st, axis = 0)\n",
    "    return sq, sv, sticker, undo_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "t_sq, t_sv, t_sticker, t_undo_sort = sort_buckets(\n",
    "    t_buckets, t_q, t_v, t_n_buckets, t_n_hashes, t_seqlen\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DeviceArray([ 0,  4,  1,  5,  2,  6,  3,  7,  8, 12,  9, 13, 10, 14, 11,\n",
       "             15], dtype=int32)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "t_undo_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = np.arange(16 * 3).reshape((16, 3))\n",
    "b = np.arange(16 * 5).reshape((16, 5))\n",
    "chunksize = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3,  4],\n",
       "       [ 5,  6,  7,  8,  9],\n",
       "       [10, 11, 12, 13, 14],\n",
       "       [15, 16, 17, 18, 19],\n",
       "       [20, 21, 22, 23, 24],\n",
       "       [25, 26, 27, 28, 29],\n",
       "       [30, 31, 32, 33, 34],\n",
       "       [35, 36, 37, 38, 39],\n",
       "       [40, 41, 42, 43, 44],\n",
       "       [45, 46, 47, 48, 49],\n",
       "       [50, 51, 52, 53, 54],\n",
       "       [55, 56, 57, 58, 59],\n",
       "       [60, 61, 62, 63, 64],\n",
       "       [65, 66, 67, 68, 69],\n",
       "       [70, 71, 72, 73, 74],\n",
       "       [75, 76, 77, 78, 79]])"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsq = np.reshape(a, (-1, chunksize, a.shape[-1]))\n",
    "rsqt = np.swapaxes(rsq, -1, -2)\n",
    "dotlike = np.matmul(rsq,rsqt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vr = np.reshape(b, (-1, chunksize, b.shape[-1]))\n",
    "so = np.matmul(dotlike,vr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(8, 2, 2) (8, 2, 5)\n"
     ]
    }
   ],
   "source": [
    "print(dotlike.shape,vr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[2] [3] [4]\nname: add1 number of inputs: 1 number of outputs: 1\n"
     ]
    }
   ],
   "source": [
    "# simple function taking one input and one output\n",
    "bl_add1 = tl.Fn(\"add1\", lambda x0: (x0 + 1), n_out=1)\n",
    "bl_add2 = tl.Fn(\"add2\", lambda x0: (x0 + 2), n_out=1)\n",
    "bl_add3 = tl.Fn(\"add3\", lambda x0: (x0 + 3), n_out=1)\n",
    "# try them out\n",
    "x = np.array([1])\n",
    "print(bl_add1(x), bl_add2(x), bl_add3(x))\n",
    "# some information about our new layers\n",
    "print(\n",
    "    \"name:\",\n",
    "    bl_add1.name,\n",
    "    \"number of inputs:\",\n",
    "    bl_add1.n_in,\n",
    "    \"number of outputs:\",\n",
    "    bl_add1.n_out,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trax.models.reformer.reformer import ReversibleHalfResidualV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Help on class ReversibleHalfResidualV2 in module trax.models.reformer.reformer:\n\nclass ReversibleHalfResidualV2(trax.layers.reversible.ReversibleLayer)\n |  ReversibleHalfResidualV2(*residual_layers, attention_layer=None)\n |  \n |  Half of a RevNet-style residual that optionally performs attention.\n |  \n |  When attention_layer is None, this layer has the signature ::\n |  \n |      [accumulator, *context] -> [accumulator + f(context), *context]\n |  \n |  The attention_layer must be an instance of EfficientAttentionBase or one of\n |  its subclasses (see efficient_attention.py), or None.\n |  \n |  Attention is special-cased for the following two reasons:\n |  \n |  - LSH attention needs to save bucket assignments from the forward pass to the\n |    backward pass, for training stability. This requires special-casing it.\n |  - We can call attention_layer.forward_and_or_backward to compute its output\n |    (needed for inverting a reversible residual layer) while simultaneously\n |    performing the backward pass. Sharing computation between these two\n |    operations improves training speed.\n |  \n |  Method resolution order:\n |      ReversibleHalfResidualV2\n |      trax.layers.reversible.ReversibleLayer\n |      trax.layers.base.Layer\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __init__(self, *residual_layers, attention_layer=None)\n |      Creates a partially initialized, unconnected layer instance.\n |      \n |      Args:\n |        n_in: Number of inputs expected by this layer.\n |        n_out: Number of outputs promised by this layer.\n |        name: Class-like name for this layer; for use when printing this layer.\n |        sublayers_to_print: Sublayers to display when printing out this layer;\n |          By default (when None) we display all sublayers.\n |  \n |  forward(self, xs)\n |      Computes this layer's output as part of a forward pass through the model.\n |      \n |      Authors of new layer subclasses should override this method to define the\n |      forward computation that their layer performs. Use `self.weights` to access\n |      trainable weights of this layer. If you need to use local non-trainable\n |      state or randomness, use `self.rng` for the random seed (no need to set it)\n |      and use `self.state` for non-trainable state (and set it to the new value).\n |      \n |      Args:\n |        inputs: Zero or more input tensors, packaged as described in the `Layer`\n |            class docstring.\n |      \n |      Returns:\n |        Zero or more output tensors, packaged as described in the `Layer` class\n |        docstring.\n |  \n |  init_weights_and_state(self, input_signature)\n |      Initializes weights and state for inputs with the given signature.\n |      \n |      Authors of new layer subclasses should override this method if their layer\n |      uses trainable weights or non-trainable state. To initialize trainable\n |      weights, set `self.weights` and to initialize non-trainable state,\n |      set `self.state` to the intended value.\n |      \n |      Args:\n |        input_signature: A `ShapeDtype` instance (if this layer takes one input)\n |            or a list/tuple of `ShapeDtype` instances; signatures of inputs.\n |  \n |  reverse(self, output, weights=(), state=(), new_state=(), rng=None)\n |      Reverse this layer: compute input given output.\n |  \n |  reverse_and_grad(self, output, ct, weights=(), state=(), new_state=(), rng=None)\n |      Backward pass: computes the inverse of a layer and propagates gradients.\n |      \n |      While you may choose to only implement reverse, some layers implement this\n |      function directly as computation may be shared between reversing and\n |      computing gradients.\n |      \n |      Args:\n |        output: Output activations; can be a (possibly nested) tuple.\n |        grad: gradient signal (cotangent) computed based on subsequent layers.\n |          The structure and shape must match the output.\n |        weights: layer weights\n |        state: start state\n |        new_state: updated state computed by the forward pass\n |        rng: Single-use random number generator (JAX PRNG key).\n |      \n |      Returns:\n |        A tuple (x, (x_grad, weights_grad)), where x is the reconstructed input,\n |        x_grad is the gradient signal for the input, and weights_grad is the\n |        gradient signal for the weights.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from trax.layers.reversible.ReversibleLayer:\n |  \n |  backward(self, inputs, output, grad, weights, state, new_state, rng)\n |      Custom backward pass to propagate gradients in a custom way.\n |      \n |      Args:\n |        inputs: Input tensors; can be a (possibly nested) tuple.\n |        output: The result of running this layer on inputs.\n |        grad: Gradient signal computed based on subsequent layers; its structure\n |            and shape must match output.\n |        weights: This layer's weights.\n |        state: This layer's state prior to the current forward pass.\n |        new_state: This layer's state after the current forward pass.\n |        rng: Single-use random number generator (JAX PRNG key).\n |      \n |      Returns:\n |        The custom gradient signal for the input. Note that we need to return\n |        a gradient for each argument of forward, so it will usually be a tuple\n |        of signals: the gradient for inputs and weights.\n |  \n |  ----------------------------------------------------------------------\n |  Readonly properties inherited from trax.layers.reversible.ReversibleLayer:\n |  \n |  has_backward\n |      Returns `True` if this layer provides its own custom backward pass code.\n |      \n |      A layer subclass that provides custom backward pass code (for custom\n |      gradients) must override this method to return `True`.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from trax.layers.base.Layer:\n |  \n |  __call__(self, x, weights=None, state=None, rng=None)\n |      Makes layers callable; for use in tests or interactive settings.\n |      \n |      This convenience method helps library users play with, test, or otherwise\n |      probe the behavior of layers outside of a full training environment. It\n |      presents the layer as callable function from inputs to outputs, with the\n |      option of manually specifying weights and non-parameter state per individual\n |      call. For convenience, weights and non-parameter state are cached per layer\n |      instance, starting from default values of `EMPTY_WEIGHTS` and `EMPTY_STATE`,\n |      and acquiring non-empty values either by initialization or from values\n |      explicitly provided via the weights and state keyword arguments.\n |      \n |      Args:\n |        x: Zero or more input tensors, packaged as described in the `Layer` class\n |            docstring.\n |        weights: Weights or `None`; if `None`, use self's cached weights value.\n |        state: State or `None`; if `None`, use self's cached state value.\n |        rng: Single-use random number generator (JAX PRNG key), or `None`;\n |            if `None`, use a default computed from an integer 0 seed.\n |      \n |      Returns:\n |        Zero or more output tensors, packaged as described in the `Layer` class\n |        docstring.\n |  \n |  __repr__(self)\n |      Return repr(self).\n |  \n |  init(self, input_signature, rng=None, use_cache=False)\n |      Initializes weights/state of this layer and its sublayers recursively.\n |      \n |      Initialization creates layer weights and state, for layers that use them.\n |      It derives the necessary array shapes and data types from the layer's input\n |      signature, which is itself just shape and data type information.\n |      \n |      For layers without weights or state, this method safely does nothing.\n |      \n |      This method is designed to create weights/state only once for each layer\n |      instance, even if the same layer instance occurs in multiple places in the\n |      network. This enables weight sharing to be implemented as layer sharing.\n |      \n |      Args:\n |        input_signature: `ShapeDtype` instance (if this layer takes one input)\n |            or list/tuple of `ShapeDtype` instances.\n |        rng: Single-use random number generator (JAX PRNG key), or `None`;\n |            if `None`, use a default computed from an integer 0 seed.\n |        use_cache: If `True`, and if this layer instance has already been\n |            initialized elsewhere in the network, then return special marker\n |            values -- tuple `(GET_WEIGHTS_FROM_CACHE, GET_STATE_FROM_CACHE)`.\n |            Else return this layer's newly initialized weights and state.\n |      \n |      Returns:\n |        A `(weights, state)` tuple.\n |  \n |  init_from_file(self, file_name, weights_only=False, input_signature=None)\n |      Initializes this layer and its sublayers from a pickled checkpoint.\n |      \n |      In the common case (`weights_only=False`), the file must be a gziped pickled\n |      dictionary containing items with keys `'flat_weights', `'flat_state'` and\n |      `'input_signature'`, which are used to initialize this layer.\n |      If `input_signature` is specified, it's used instead of the one in the file.\n |      If `weights_only` is `True`, the dictionary does not need to have the\n |      `'flat_state'` item and the state it not restored either.\n |      \n |      Args:\n |        file_name: Name/path of the pickeled weights/state file.\n |        weights_only: If `True`, initialize only the layer's weights. Else\n |            initialize both weights and state.\n |        input_signature: Input signature to be used instead of the one from file.\n |  \n |  output_signature(self, input_signature)\n |      Returns output signature this layer would give for `input_signature`.\n |  \n |  pure_fn(self, x, weights, state, rng, use_cache=False)\n |      Applies this layer as a pure function with no optional args.\n |      \n |      This method exposes the layer's computation as a pure function. This is\n |      especially useful for JIT compilation. Do not override, use `forward`\n |      instead.\n |      \n |      Args:\n |        x: Zero or more input tensors, packaged as described in the `Layer` class\n |            docstring.\n |        weights: A tuple or list of trainable weights, with one element for this\n |            layer if this layer has no sublayers, or one for each sublayer if\n |            this layer has sublayers. If a layer (or sublayer) has no trainable\n |            weights, the corresponding weights element is an empty tuple.\n |        state: Layer-specific non-parameter state that can update between batches.\n |        rng: Single-use random number generator (JAX PRNG key).\n |        use_cache: if `True`, cache weights and state in the layer object; used\n |          to implement layer sharing in combinators.\n |      \n |      Returns:\n |        A tuple of `(tensors, state)`. The tensors match the number (`n_out`)\n |        promised by this layer, and are packaged as described in the `Layer`\n |        class docstring.\n |  \n |  weights_and_state_signature(self, input_signature)\n |      Return a pair containing the signatures of weights and state.\n |  \n |  ----------------------------------------------------------------------\n |  Readonly properties inherited from trax.layers.base.Layer:\n |  \n |  n_in\n |      Returns how many tensors this layer expects as input.\n |  \n |  n_out\n |      Returns how many tensors this layer promises as output.\n |  \n |  name\n |      Returns the name of this layer.\n |  \n |  sublayers\n |      Returns a tuple containing this layer's sublayers; may be empty.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from trax.layers.base.Layer:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  rng\n |      Returns a single-use random number generator without advancing it.\n |  \n |  state\n |      Returns a tuple containing this layer's state; may be empty.\n |      \n |      If the layer has sublayers, the state by convention will be\n |      a tuple of length `len(sublayers)` containing sublayer states.\n |      Note that in this case self._state only marks which ones are shared.\n |  \n |  weights\n |      Returns this layer's weights.\n |      \n |      Depending on the layer, the weights can be in the form of:\n |      \n |        - an empty tuple\n |        - a tensor (ndarray)\n |        - a nested structure of tuples and tensors\n |      \n |      If the layer has sublayers, the weights by convention will be\n |      a tuple of length `len(sublayers)` containing the weights of sublayers.\n |      Note that in this case self._weights only marks which ones are shared.\n\n"
     ]
    }
   ],
   "source": [
    "help(ReversibleHalfResidualV2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Help on class ReversibleSerial in module trax.layers.reversible:\n\nclass ReversibleSerial(ReversibleLayer, trax.layers.combinators.Serial)\n |  ReversibleSerial(*layers)\n |  \n |  A reversible version of tl.Serial (requires reversible sub-layers).\n |  \n |  Method resolution order:\n |      ReversibleSerial\n |      ReversibleLayer\n |      trax.layers.combinators.Serial\n |      trax.layers.base.Layer\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __init__(self, *layers)\n |      Creates a partially initialized, unconnected layer instance.\n |      \n |      Args:\n |        n_in: Number of inputs expected by this layer.\n |        n_out: Number of outputs promised by this layer.\n |        name: Class-like name for this layer; for use when printing this layer.\n |        sublayers_to_print: Sublayers to display when printing out this layer;\n |          By default (when None) we display all sublayers.\n |  \n |  reverse(self, output, weights=(), state=(), new_state=(), rng=None)\n |      Reverse this layer: compute input given output.\n |  \n |  reverse_and_grad(self, output, grad, weights=(), state=(), new_state=(), rng=None)\n |      Backward pass: computes the inverse of a layer and propagates gradients.\n |      \n |      While you may choose to only implement reverse, some layers implement this\n |      function directly as computation may be shared between reversing and\n |      computing gradients.\n |      \n |      Args:\n |        output: Output activations; can be a (possibly nested) tuple.\n |        grad: gradient signal (cotangent) computed based on subsequent layers.\n |          The structure and shape must match the output.\n |        weights: layer weights\n |        state: start state\n |        new_state: updated state computed by the forward pass\n |        rng: Single-use random number generator (JAX PRNG key).\n |      \n |      Returns:\n |        A tuple (x, (x_grad, weights_grad)), where x is the reconstructed input,\n |        x_grad is the gradient signal for the input, and weights_grad is the\n |        gradient signal for the weights.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from ReversibleLayer:\n |  \n |  backward(self, inputs, output, grad, weights, state, new_state, rng)\n |      Custom backward pass to propagate gradients in a custom way.\n |      \n |      Args:\n |        inputs: Input tensors; can be a (possibly nested) tuple.\n |        output: The result of running this layer on inputs.\n |        grad: Gradient signal computed based on subsequent layers; its structure\n |            and shape must match output.\n |        weights: This layer's weights.\n |        state: This layer's state prior to the current forward pass.\n |        new_state: This layer's state after the current forward pass.\n |        rng: Single-use random number generator (JAX PRNG key).\n |      \n |      Returns:\n |        The custom gradient signal for the input. Note that we need to return\n |        a gradient for each argument of forward, so it will usually be a tuple\n |        of signals: the gradient for inputs and weights.\n |  \n |  ----------------------------------------------------------------------\n |  Readonly properties inherited from ReversibleLayer:\n |  \n |  has_backward\n |      Returns `True` if this layer provides its own custom backward pass code.\n |      \n |      A layer subclass that provides custom backward pass code (for custom\n |      gradients) must override this method to return `True`.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from trax.layers.combinators.Serial:\n |  \n |  forward(self, xs)\n |      Computes this layer's output as part of a forward pass through the model.\n |      \n |      Authors of new layer subclasses should override this method to define the\n |      forward computation that their layer performs. Use `self.weights` to access\n |      trainable weights of this layer. If you need to use local non-trainable\n |      state or randomness, use `self.rng` for the random seed (no need to set it)\n |      and use `self.state` for non-trainable state (and set it to the new value).\n |      \n |      Args:\n |        inputs: Zero or more input tensors, packaged as described in the `Layer`\n |            class docstring.\n |      \n |      Returns:\n |        Zero or more output tensors, packaged as described in the `Layer` class\n |        docstring.\n |  \n |  init_weights_and_state(self, input_signature)\n |      Initializes weights and state for inputs with the given signature.\n |      \n |      Authors of new layer subclasses should override this method if their layer\n |      uses trainable weights or non-trainable state. To initialize trainable\n |      weights, set `self.weights` and to initialize non-trainable state,\n |      set `self.state` to the intended value.\n |      \n |      Args:\n |        input_signature: A `ShapeDtype` instance (if this layer takes one input)\n |            or a list/tuple of `ShapeDtype` instances; signatures of inputs.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from trax.layers.base.Layer:\n |  \n |  __call__(self, x, weights=None, state=None, rng=None)\n |      Makes layers callable; for use in tests or interactive settings.\n |      \n |      This convenience method helps library users play with, test, or otherwise\n |      probe the behavior of layers outside of a full training environment. It\n |      presents the layer as callable function from inputs to outputs, with the\n |      option of manually specifying weights and non-parameter state per individual\n |      call. For convenience, weights and non-parameter state are cached per layer\n |      instance, starting from default values of `EMPTY_WEIGHTS` and `EMPTY_STATE`,\n |      and acquiring non-empty values either by initialization or from values\n |      explicitly provided via the weights and state keyword arguments.\n |      \n |      Args:\n |        x: Zero or more input tensors, packaged as described in the `Layer` class\n |            docstring.\n |        weights: Weights or `None`; if `None`, use self's cached weights value.\n |        state: State or `None`; if `None`, use self's cached state value.\n |        rng: Single-use random number generator (JAX PRNG key), or `None`;\n |            if `None`, use a default computed from an integer 0 seed.\n |      \n |      Returns:\n |        Zero or more output tensors, packaged as described in the `Layer` class\n |        docstring.\n |  \n |  __repr__(self)\n |      Return repr(self).\n |  \n |  init(self, input_signature, rng=None, use_cache=False)\n |      Initializes weights/state of this layer and its sublayers recursively.\n |      \n |      Initialization creates layer weights and state, for layers that use them.\n |      It derives the necessary array shapes and data types from the layer's input\n |      signature, which is itself just shape and data type information.\n |      \n |      For layers without weights or state, this method safely does nothing.\n |      \n |      This method is designed to create weights/state only once for each layer\n |      instance, even if the same layer instance occurs in multiple places in the\n |      network. This enables weight sharing to be implemented as layer sharing.\n |      \n |      Args:\n |        input_signature: `ShapeDtype` instance (if this layer takes one input)\n |            or list/tuple of `ShapeDtype` instances.\n |        rng: Single-use random number generator (JAX PRNG key), or `None`;\n |            if `None`, use a default computed from an integer 0 seed.\n |        use_cache: If `True`, and if this layer instance has already been\n |            initialized elsewhere in the network, then return special marker\n |            values -- tuple `(GET_WEIGHTS_FROM_CACHE, GET_STATE_FROM_CACHE)`.\n |            Else return this layer's newly initialized weights and state.\n |      \n |      Returns:\n |        A `(weights, state)` tuple.\n |  \n |  init_from_file(self, file_name, weights_only=False, input_signature=None)\n |      Initializes this layer and its sublayers from a pickled checkpoint.\n |      \n |      In the common case (`weights_only=False`), the file must be a gziped pickled\n |      dictionary containing items with keys `'flat_weights', `'flat_state'` and\n |      `'input_signature'`, which are used to initialize this layer.\n |      If `input_signature` is specified, it's used instead of the one in the file.\n |      If `weights_only` is `True`, the dictionary does not need to have the\n |      `'flat_state'` item and the state it not restored either.\n |      \n |      Args:\n |        file_name: Name/path of the pickeled weights/state file.\n |        weights_only: If `True`, initialize only the layer's weights. Else\n |            initialize both weights and state.\n |        input_signature: Input signature to be used instead of the one from file.\n |  \n |  output_signature(self, input_signature)\n |      Returns output signature this layer would give for `input_signature`.\n |  \n |  pure_fn(self, x, weights, state, rng, use_cache=False)\n |      Applies this layer as a pure function with no optional args.\n |      \n |      This method exposes the layer's computation as a pure function. This is\n |      especially useful for JIT compilation. Do not override, use `forward`\n |      instead.\n |      \n |      Args:\n |        x: Zero or more input tensors, packaged as described in the `Layer` class\n |            docstring.\n |        weights: A tuple or list of trainable weights, with one element for this\n |            layer if this layer has no sublayers, or one for each sublayer if\n |            this layer has sublayers. If a layer (or sublayer) has no trainable\n |            weights, the corresponding weights element is an empty tuple.\n |        state: Layer-specific non-parameter state that can update between batches.\n |        rng: Single-use random number generator (JAX PRNG key).\n |        use_cache: if `True`, cache weights and state in the layer object; used\n |          to implement layer sharing in combinators.\n |      \n |      Returns:\n |        A tuple of `(tensors, state)`. The tensors match the number (`n_out`)\n |        promised by this layer, and are packaged as described in the `Layer`\n |        class docstring.\n |  \n |  weights_and_state_signature(self, input_signature)\n |      Return a pair containing the signatures of weights and state.\n |  \n |  ----------------------------------------------------------------------\n |  Readonly properties inherited from trax.layers.base.Layer:\n |  \n |  n_in\n |      Returns how many tensors this layer expects as input.\n |  \n |  n_out\n |      Returns how many tensors this layer promises as output.\n |  \n |  name\n |      Returns the name of this layer.\n |  \n |  sublayers\n |      Returns a tuple containing this layer's sublayers; may be empty.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from trax.layers.base.Layer:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  rng\n |      Returns a single-use random number generator without advancing it.\n |  \n |  state\n |      Returns a tuple containing this layer's state; may be empty.\n |      \n |      If the layer has sublayers, the state by convention will be\n |      a tuple of length `len(sublayers)` containing sublayer states.\n |      Note that in this case self._state only marks which ones are shared.\n |  \n |  weights\n |      Returns this layer's weights.\n |      \n |      Depending on the layer, the weights can be in the form of:\n |      \n |        - an empty tuple\n |        - a tensor (ndarray)\n |        - a nested structure of tuples and tensors\n |      \n |      If the layer has sublayers, the weights by convention will be\n |      a tuple of length `len(sublayers)` containing the weights of sublayers.\n |      Note that in this case self._weights only marks which ones are shared.\n\n"
     ]
    }
   ],
   "source": [
    "help(tl.ReversibleSerial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Help on function Dup in module trax.layers.combinators:\n\nDup()\n    Duplicates (copies) the top element on the data stack.\n\n"
     ]
    }
   ],
   "source": [
    "help(tl.Dup)"
   ]
  }
 ]
}