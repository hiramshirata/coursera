{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('env')",
   "metadata": {
    "interpreter": {
     "hash": "7a2aba773768f3028cfd68a875299321972753977854c0935b697eedbb83aab0"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "one_hot() missing 1 required positional argument: 'n_categories'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-df369a85f4d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtrax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'trax'"
     ]
    }
   ],
   "source": [
    "a = [*range(10)]\n",
    "a = np.array(a)\n",
    "\n",
    "b =  tl.one_hot(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Help on function Softmax in module trax.layers.core:\n\nSoftmax(axis=-1)\n    Returns a layer that applies softmax along one tensor axis.\n    \n    `Softmax` acts on a group of values and normalizes them to look like a set\n    of probability values. (Probability values must be non-negative, and as a\n    set must sum to 1.)\n    \n    Args:\n      axis: Axis along which values are grouped for computing softmax.\n\n"
     ]
    }
   ],
   "source": [
    "help(tl.Softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "[0]*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0, 0],\n",
       "       [0, 0]])"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = [*range(2)]\n",
    "b = np.full((2,2),0)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: evaluate_prediction\n",
    "def evaluate_prediction(pred, labels, pad):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        pred: prediction array with shape \n",
    "            (num examples, max sentence length in batch, num of classes)\n",
    "        labels: array of size (batch_size, seq_len)\n",
    "        pad: integer representing pad character\n",
    "    Outputs:\n",
    "        accuracy: float\n",
    "    \"\"\"\n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "## step 1 ##\n",
    "    outputs = np.array([ [np.argmax(i) for i in row] for row in pred])\n",
    "    print(\"outputs shape:\", outputs.shape)\n",
    "\n",
    "## step 2 ##\n",
    "    mask = outputs == labels\n",
    "    print(\"mask shape:\", mask.shape, \"mask[0][20:30]:\", mask[0][20:30])\n",
    "## step 3 ##\n",
    "    accuracy = np.sum(mask)/np.sum(y != pad)\n",
    "    ### END CODE HERE ###\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "499999500000"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: evaluate_prediction\n",
    "def evaluate_prediction(pred, labels, pad):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        pred: prediction array with shape \n",
    "            (num examples, max sentence length in batch, num of classes)\n",
    "        labels: array of size (batch_size, seq_len)\n",
    "        pad: integer representing pad character\n",
    "    Outputs:\n",
    "        accuracy: float\n",
    "    \"\"\"\n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "## step 1 ##\n",
    "    outputs = np.array([ [np.argmax(i) for i in row] for row in pred])\n",
    "    print(\"outputs shape:\", outputs.shape)\n",
    "\n",
    "## step 2 ##\n",
    "    mask = outputs == labels\n",
    "    print(\"mask shape:\", mask.shape, \"mask[0][20:30]:\", mask[0][20:30])\n",
    "## step 3 ##\n",
    "    accuracy = np.sum(mask)/np.sum(y != pad)\n",
    "    ### END CODE HERE ###\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.sum((np.argmax(pred,axis=-1) == labels))/float(np.sum(mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trax\n",
    "from trax import layers as tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Help on function detokenize in module trax.data.tf_inputs:\n\ndetokenize(x, vocab_type='subword', vocab_file=None, vocab_dir=None, n_reserved_ids=0)\n    Maps integer arrays to text; the opposite of `tokenize`.\n    \n    In many cases (all char- and subword-type vocabularies and most sentencepiece\n    ones) the tokenization is invertible, so detokenize(tokenize(x)) = x. In some\n    more rare cases this can remove some spacing, but it is still often useful\n    to run detokenize to get a readable version for a tokenized string.\n    \n    Args:\n      x: a list or numpy array of integers.\n      vocab_type: Type of vocabulary, one of: 'subword', 'sentencepiece', 'char'.\n      vocab_file: Name of the vocabulary file.\n      vocab_dir: Directory which contains the vocabulary file.\n      n_reserved_ids: An int, offset added so 0, ..., n_reserved_ids-1 are unused;\n        This is common for example when reserving the 0 for padding and 1 for EOS,\n        but it's only needed if these symbols are not already included (and thus\n        reserved) in the vocab_file.\n    \n    Returns:\n      A string corresponding to the de-tokenized version of x.\n\n"
     ]
    }
   ],
   "source": [
    "help(trax.data.detokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Help on function ShiftRight in module trax.layers.attention:\n\nShiftRight(n_positions=1, mode='train')\n    Returns a layer that can insert padding to shift the input sequence.\n    \n    Args:\n      n_positions: Number of positions to shift the input sequence rightward;\n          initial positions freed by the shift get padded with zeros.\n      mode: One of `'train'`, `'eval'`, or `'predict'`.\n\n"
     ]
    }
   ],
   "source": [
    "help(tl.ShiftRight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([[[0], [1], [2]]])\n",
    "np.squeeze(a)"
   ]
  }
 ]
}